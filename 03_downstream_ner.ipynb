{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt selection and testing\n",
    "\n",
    "This notebook chooses the most appropriate prompt and prompt structure for the OCR correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import config  # Import your config.py file this contains you openai api key\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from llm_comparison_toolkit import RateLimiter, get_response_openai, get_response_anthropic,  create_config_dict_func, compare_request_configurations, generate_model_configs\n",
    "from evaluate import load\n",
    "from evaluation_funcs import evaluate_correction_performance, evaluate_correction_performance_folders, get_metric_error_reduction\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_functions import files_to_df_func, evaluate_ner, calculate_entity_similarity, repeat_prompt_experiment\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "\n",
    "dev_data_folder = 'data/dev_data'\n",
    "dev_transcripts = os.path.join(dev_data_folder, 'dev_data_transcript')\n",
    "dev_raw_ocr_folder =  os.path.join(dev_data_folder,'dev_raw_ocr' )\n",
    "dev_system_message_folder = os.path.join(dev_data_folder,'dev_system_message_variants' )\n",
    "\n",
    "#load the dev and test sets for prompt development and selection\n",
    "dev_data_df = pd.read_csv(os.path.join(dev_data_folder,'dev_data_raw.csv'))\n",
    "\n",
    "\n",
    "#for saving data to be used in the analysis\n",
    "if not os.path.exists('data/analysis'):\n",
    "    os.makedirs('data/analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate performance on downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d646692a0a400699875b5ad172c361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/400 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97c8f3537201456484e40db9841f21a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27088657b314ee98ec68393c57daf66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/8.66M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64d765e2d59e415f9db0a1d6e88ecc56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203c6fd54d8149ceba0976583ba7802b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd675ce540984f43b25e8647f8151303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c79acb3ee94a1482a7d287b789beb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "ner_model = \"Gladiator/microsoft-deberta-v3-large_ner_conll2003\"#\"dslim/bert-base-NER\"\n",
    "\n",
    "transcribed_data_set_df = files_to_df_func(transcribed_files)\n",
    "raw_data_set_df = files_to_df_func(raw_folder)\n",
    "\n",
    "raw_data_set_df = raw_data_set_df.loc[raw_data_set_df['file_name'].isin(transcribed_data_set_df['file_name'])]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ner_model)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(ner_model)\n",
    "def preprocess_text(text):\n",
    "    # Example preprocessing: substitute multiple whitespaces with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "perform_ner = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def perform_ner_on_text(text, ner_pipeline):\n",
    "    # Preprocess the text first\n",
    "    preprocessed_text = preprocess_text(text)\n",
    "    # Then pass the preprocessed text to the NER pipeline\n",
    "    return ner_pipeline(preprocessed_text)\n",
    "\n",
    "transcribed_data_set_df['ner_results'] = transcribed_data_set_df['content'].apply(perform_ner_on_text, ner_pipeline=perform_ner)\n",
    "\n",
    "raw_data_set_df['ner_results_raw'] = raw_data_set_df['content'].apply(perform_ner_on_text, ner_pipeline=perform_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median cosine similarity: 0.92\n"
     ]
    }
   ],
   "source": [
    "#load on the transcribed data... this should be done within a loop as there are many models to test\n",
    "LM_corrected_df = files_to_df_func(os.path.join(corrected_folder, 'claude_temp_claude-3-opus-20240229'))\n",
    "temp_ner = transcribed_data_set_df.copy().merge(LM_corrected_df.loc[:,['content', 'file_name'] ].rename(columns={'content':'content_corrected'}), \n",
    "                                                on = 'file_name')\n",
    "\n",
    "temp_ner['ner_results_corrected'] = temp_ner['content_corrected'].apply(perform_ner_on_text, ner_pipeline=perform_ner)\n",
    "\n",
    "temp_ner['cosine_sim'] = temp_ner.apply(lambda row: calculate_entity_similarity(row['ner_results_corrected'],row['ner_results']), axis = 1)\n",
    "\n",
    "\n",
    "print(\"Median cosine similarity:\", temp_ner['cosine_sim'].median().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ner_results_corrected'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/redigitalize/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ner_results_corrected'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results, results_by_tag \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_ner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mner_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mner_results_corrected\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(results_by_tag)\n",
      "File \u001b[0;32m~/redigitalize/helper_functions.py:263\u001b[0m, in \u001b[0;36mevaluate_ner\u001b[0;34m(df, truth, predictions)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03mEvaluate the Named Entity Recognition (NER) predictions against the ground truth labels\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03mcontained in a dataframe. This function computes the performance metrics for NER tasks,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;124;03m    >>> print(detailed_results)\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    262\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m df[truth]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m--> 263\u001b[0m predicted \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m    265\u001b[0m ground_truth_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    266\u001b[0m predicted_labels \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/redigitalize/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/redigitalize/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ner_results_corrected'"
     ]
    }
   ],
   "source": [
    "results, results_by_tag = evaluate_ner(temp_ner, 'ner_results', 'ner_results_corrected')\n",
    "\n",
    "print(results)\n",
    "print(results_by_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median cosine similarity: 0.74\n",
      "{'ent_type': {'correct': 97, 'incorrect': 63, 'partial': 0, 'missed': 588, 'spurious': 545, 'possible': 748, 'actual': 705, 'precision': 0.1375886524822695, 'recall': 0.12967914438502673, 'f1': 0.1335168616655196}, 'partial': {'correct': 16, 'incorrect': 0, 'partial': 144, 'missed': 588, 'spurious': 545, 'possible': 748, 'actual': 705, 'precision': 0.12482269503546099, 'recall': 0.11764705882352941, 'f1': 0.12112869924294564}, 'strict': {'correct': 14, 'incorrect': 146, 'partial': 0, 'missed': 588, 'spurious': 545, 'possible': 748, 'actual': 705, 'precision': 0.019858156028368795, 'recall': 0.01871657754010695, 'f1': 0.019270474879559532}, 'exact': {'correct': 16, 'incorrect': 144, 'partial': 0, 'missed': 588, 'spurious': 545, 'possible': 748, 'actual': 705, 'precision': 0.02269503546099291, 'recall': 0.0213903743315508, 'f1': 0.022023399862353753}}\n",
      "{'B-LOC': {'ent_type': {'correct': 28, 'incorrect': 15, 'partial': 0, 'missed': 84, 'spurious': 78, 'possible': 127, 'actual': 121, 'precision': 0.23140495867768596, 'recall': 0.2204724409448819, 'f1': 0.22580645161290325}, 'partial': {'correct': 4, 'incorrect': 0, 'partial': 39, 'missed': 84, 'spurious': 78, 'possible': 127, 'actual': 121, 'precision': 0.19421487603305784, 'recall': 0.18503937007874016, 'f1': 0.18951612903225803}, 'strict': {'correct': 2, 'incorrect': 41, 'partial': 0, 'missed': 84, 'spurious': 78, 'possible': 127, 'actual': 121, 'precision': 0.01652892561983471, 'recall': 0.015748031496062992, 'f1': 0.016129032258064516}, 'exact': {'correct': 4, 'incorrect': 39, 'partial': 0, 'missed': 84, 'spurious': 78, 'possible': 127, 'actual': 121, 'precision': 0.03305785123966942, 'recall': 0.031496062992125984, 'f1': 0.03225806451612903}}, 'I-LOC': {'ent_type': {'correct': 6, 'incorrect': 6, 'partial': 0, 'missed': 67, 'spurious': 44, 'possible': 79, 'actual': 56, 'precision': 0.10714285714285714, 'recall': 0.0759493670886076, 'f1': 0.08888888888888889}, 'partial': {'correct': 0, 'incorrect': 0, 'partial': 12, 'missed': 67, 'spurious': 44, 'possible': 79, 'actual': 56, 'precision': 0.10714285714285714, 'recall': 0.0759493670886076, 'f1': 0.08888888888888889}, 'strict': {'correct': 0, 'incorrect': 12, 'partial': 0, 'missed': 67, 'spurious': 44, 'possible': 79, 'actual': 56, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'exact': {'correct': 0, 'incorrect': 12, 'partial': 0, 'missed': 67, 'spurious': 44, 'possible': 79, 'actual': 56, 'precision': 0.0, 'recall': 0.0, 'f1': 0}}, 'B-MISC': {'ent_type': {'correct': 8, 'incorrect': 10, 'partial': 0, 'missed': 74, 'spurious': 79, 'possible': 92, 'actual': 97, 'precision': 0.08247422680412371, 'recall': 0.08695652173913043, 'f1': 0.08465608465608467}, 'partial': {'correct': 0, 'incorrect': 0, 'partial': 18, 'missed': 74, 'spurious': 79, 'possible': 92, 'actual': 97, 'precision': 0.09278350515463918, 'recall': 0.09782608695652174, 'f1': 0.09523809523809525}, 'strict': {'correct': 0, 'incorrect': 18, 'partial': 0, 'missed': 74, 'spurious': 79, 'possible': 92, 'actual': 97, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'exact': {'correct': 0, 'incorrect': 18, 'partial': 0, 'missed': 74, 'spurious': 79, 'possible': 92, 'actual': 97, 'precision': 0.0, 'recall': 0.0, 'f1': 0}}, 'I-MISC': {'ent_type': {'correct': 2, 'incorrect': 4, 'partial': 0, 'missed': 51, 'spurious': 46, 'possible': 57, 'actual': 52, 'precision': 0.038461538461538464, 'recall': 0.03508771929824561, 'f1': 0.03669724770642201}, 'partial': {'correct': 0, 'incorrect': 0, 'partial': 6, 'missed': 51, 'spurious': 46, 'possible': 57, 'actual': 52, 'precision': 0.057692307692307696, 'recall': 0.05263157894736842, 'f1': 0.05504587155963303}, 'strict': {'correct': 0, 'incorrect': 6, 'partial': 0, 'missed': 51, 'spurious': 46, 'possible': 57, 'actual': 52, 'precision': 0.0, 'recall': 0.0, 'f1': 0}, 'exact': {'correct': 0, 'incorrect': 6, 'partial': 0, 'missed': 51, 'spurious': 46, 'possible': 57, 'actual': 52, 'precision': 0.0, 'recall': 0.0, 'f1': 0}}, 'B-ORG': {'ent_type': {'correct': 6, 'incorrect': 4, 'partial': 0, 'missed': 19, 'spurious': 17, 'possible': 29, 'actual': 27, 'precision': 0.2222222222222222, 'recall': 0.20689655172413793, 'f1': 0.2142857142857143}, 'partial': {'correct': 3, 'incorrect': 0, 'partial': 7, 'missed': 19, 'spurious': 17, 'possible': 29, 'actual': 27, 'precision': 0.24074074074074073, 'recall': 0.22413793103448276, 'f1': 0.23214285714285715}, 'strict': {'correct': 3, 'incorrect': 7, 'partial': 0, 'missed': 19, 'spurious': 17, 'possible': 29, 'actual': 27, 'precision': 0.1111111111111111, 'recall': 0.10344827586206896, 'f1': 0.10714285714285715}, 'exact': {'correct': 3, 'incorrect': 7, 'partial': 0, 'missed': 19, 'spurious': 17, 'possible': 29, 'actual': 27, 'precision': 0.1111111111111111, 'recall': 0.10344827586206896, 'f1': 0.10714285714285715}}, 'I-ORG': {'ent_type': {'correct': 12, 'incorrect': 9, 'partial': 0, 'missed': 36, 'spurious': 45, 'possible': 57, 'actual': 66, 'precision': 0.18181818181818182, 'recall': 0.21052631578947367, 'f1': 0.1951219512195122}, 'partial': {'correct': 2, 'incorrect': 0, 'partial': 19, 'missed': 36, 'spurious': 45, 'possible': 57, 'actual': 66, 'precision': 0.17424242424242425, 'recall': 0.20175438596491227, 'f1': 0.18699186991869918}, 'strict': {'correct': 2, 'incorrect': 19, 'partial': 0, 'missed': 36, 'spurious': 45, 'possible': 57, 'actual': 66, 'precision': 0.030303030303030304, 'recall': 0.03508771929824561, 'f1': 0.03252032520325203}, 'exact': {'correct': 2, 'incorrect': 19, 'partial': 0, 'missed': 36, 'spurious': 45, 'possible': 57, 'actual': 66, 'precision': 0.030303030303030304, 'recall': 0.03508771929824561, 'f1': 0.03252032520325203}}, 'B-PER': {'ent_type': {'correct': 21, 'incorrect': 14, 'partial': 0, 'missed': 123, 'spurious': 120, 'possible': 158, 'actual': 155, 'precision': 0.13548387096774195, 'recall': 0.13291139240506328, 'f1': 0.134185303514377}, 'partial': {'correct': 3, 'incorrect': 0, 'partial': 32, 'missed': 123, 'spurious': 120, 'possible': 158, 'actual': 155, 'precision': 0.12258064516129032, 'recall': 0.12025316455696203, 'f1': 0.12140575079872204}, 'strict': {'correct': 3, 'incorrect': 32, 'partial': 0, 'missed': 123, 'spurious': 120, 'possible': 158, 'actual': 155, 'precision': 0.01935483870967742, 'recall': 0.0189873417721519, 'f1': 0.01916932907348243}, 'exact': {'correct': 3, 'incorrect': 32, 'partial': 0, 'missed': 123, 'spurious': 120, 'possible': 158, 'actual': 155, 'precision': 0.01935483870967742, 'recall': 0.0189873417721519, 'f1': 0.01916932907348243}}, 'I-PER': {'ent_type': {'correct': 14, 'incorrect': 1, 'partial': 0, 'missed': 134, 'spurious': 116, 'possible': 149, 'actual': 131, 'precision': 0.10687022900763359, 'recall': 0.09395973154362416, 'f1': 0.1}, 'partial': {'correct': 4, 'incorrect': 0, 'partial': 11, 'missed': 134, 'spurious': 116, 'possible': 149, 'actual': 131, 'precision': 0.07251908396946564, 'recall': 0.06375838926174497, 'f1': 0.06785714285714285}, 'strict': {'correct': 4, 'incorrect': 11, 'partial': 0, 'missed': 134, 'spurious': 116, 'possible': 149, 'actual': 131, 'precision': 0.030534351145038167, 'recall': 0.026845637583892617, 'f1': 0.02857142857142857}, 'exact': {'correct': 4, 'incorrect': 11, 'partial': 0, 'missed': 134, 'spurious': 116, 'possible': 149, 'actual': 131, 'precision': 0.030534351145038167, 'recall': 0.026845637583892617, 'f1': 0.02857142857142857}}}\n"
     ]
    }
   ],
   "source": [
    "temp_ner = transcribed_data_set_df.copy().merge(raw_data_set_df.loc[:,['ner_results_raw', 'file_name'] ].rename(columns={'content':'content_corrected'}), \n",
    "                                                on = 'file_name')\n",
    "\n",
    "temp_ner['cosine_sim'] = temp_ner.apply(lambda row: calculate_entity_similarity(row['ner_results_raw'],row['ner_results']), axis = 1)\n",
    "\n",
    "print(\"Median cosine similarity:\", temp_ner['cosine_sim'].median().round(2))\n",
    "\n",
    "results, results_by_tag = evaluate_ner(temp_ner, 'ner_results', 'ner_results_raw')\n",
    "\n",
    "print(results)\n",
    "print(results_by_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-ORG',\n",
       "  'score': 0.95968384,\n",
       "  'index': 19,\n",
       "  'word': '▁Review',\n",
       "  'start': 89,\n",
       "  'end': 96}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_ner['ner_results'][11]#['content'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
