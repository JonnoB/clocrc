{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models\n",
    "\n",
    "This notebook chooses the most appropriate prompt and prompt structure for the OCR correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import config  # Import your config.py file this contains you openai api key\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from llm_comparison_toolkit import RateLimiter, get_response_openai, get_response_anthropic, get_response_replicate, create_config_dict_func, compare_request_configurations, generate_model_configs\n",
    "from evaluate import load\n",
    "from evaluation_funcs import evaluate_correction_performance, evaluate_correction_performance_folders, get_metric_error_reduction\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from helper_functions import files_to_df_func, files_to_df_core_func\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import re\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv()\n",
    "save_appendix = os.getenv(\"save_appendix\")\n",
    "\n",
    "\n",
    "dev_data_folder = 'data/dev_data'\n",
    "dev_transcripts = os.path.join(dev_data_folder, 'dev_data_transcript')\n",
    "dev_raw_ocr_folder =  os.path.join(dev_data_folder,'dev_raw_ocr' )\n",
    "dev_system_message_folder = os.path.join(dev_data_folder,'dev_system_message_variants' )\n",
    "\n",
    "\n",
    "#NCSE\n",
    "\n",
    "ncse_folder = 'data/transcription_returned_ocr'\n",
    "ncse_articles_raw = os.path.join(ncse_folder, 'transcription_raw_ocr')\n",
    "ncse_articles_transcribed = os.path.join(ncse_folder, 'transcription_files') \n",
    "ncse_articles_results = os.path.join(ncse_folder, 'corrected_folder')\n",
    "\n",
    "#Overproof\n",
    "overproof_folder = 'data/overproof'\n",
    "\n",
    "smh_folder =  os.path.join(overproof_folder, 'SMH')\n",
    "smh_articles_raw = os.path.join(smh_folder, 'article_level', 'raw')\n",
    "smh_articles_transcribed = os.path.join(smh_folder, 'article_level', 'corrected') #in the dataset they are reffered to as corrected but this clashes with my naming convention\n",
    "smh_articles_results = os.path.join(smh_folder, 'results')\n",
    "\n",
    "ca_folder =  os.path.join(overproof_folder, 'CA')\n",
    "ca_articles_raw = os.path.join(ca_folder, 'article_level', 'raw')\n",
    "ca_articles_transcribed = os.path.join(ca_folder, 'article_level', 'corrected') #in the dataset they are reffered to as corrected but this clashes with my naming convention\n",
    "ca_articles_results = os.path.join(ca_folder, 'results')\n",
    "\n",
    "#load the dev and test sets for prompt development and selection\n",
    "dev_data_df = pd.read_csv(os.path.join(dev_data_folder,'dev_data_raw.csv'))\n",
    "\n",
    "\n",
    "#for saving data to be used in the analysis\n",
    "if not os.path.exists('data/analysis'):\n",
    "    os.makedirs('data/analysis')\n",
    "\n",
    "\n",
    "wer = load(\"wer\")\n",
    "cer = load(\"cer\")\n",
    "\n",
    "\n",
    "model_name_code = pd.Series(\n",
    "    {'Llama 2 70B':'llama2-70b-4096',\n",
    " 'Gemma 7B':'gemma-7b-it',\n",
    " 'Claude 3 Opus':'claude-3-opus-20240229',\n",
    " 'Claude 3 Haiku':'claude-3-haiku-20240307',\n",
    " 'GPT-4':'gpt-4-turbo-preview',\n",
    " 'GPT-3.5':'gpt-3.5-turbo',\n",
    " 'Mixtral 8x7B':'mixtral-8x7b-32768',\n",
    " \"Llama 3\":'meta-llama-3-70b-instruct',\n",
    " \"Llama 3 base\":'meta/meta-llama-3-70b',\n",
    " 'Overproof':'overproof'})\n",
    "\n",
    "\n",
    "eval_metric = 'CER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate system prompt tests\n",
    "\n",
    "We evaluate the system prompts below to see if thre is any significant difference between the prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating models on the test set\n",
    "\n",
    "Having identified two different prompts and that the prompts appear to work better when places after the text we can now compare the different models\n",
    "\n",
    "\n",
    "The below code creates the basic configuration dictionaries for each model and then fills in the with the two different prompt messages creating a single list of all basic prompt/model configurations. It then calls all the LLM's and saves the results.\n",
    "This works in series so takes a while."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the prompt/system message using the best performing from the previous section\n",
    "\n",
    "full_prompt = \"{content_html}\"+f\"\"\" \\n \\n \"\"\" + f\"\"\"You are an expert in post-OCR correction of documents. Using the context available from the text please recover the most likely original text from the corrupted OCR. The text is from an english newspaper in the 1800's. The text may be an advert or article and may be missing the beggining or end. Do not add any text, commentary, or lead in sentences beyond the recovered text. Do not add a title, or any introductions.\"\"\"\n",
    "\n",
    "instruct_prompt = \"{content_html}\"+f\"\"\" \\n \\n \"\"\" + f\"\"\"You are an expert in post-OCR correction of documents. Using the context available from the text please recover the most likely original text from the corrupted OCR. Do not add any text, commentary, or lead in sentences beyond the recovered text. Do not add a title, or any introductions.\"\"\"\n",
    "\n",
    "boros_basic  = \"{content_html}\"+f\"\"\" \\n \\n \"\"\" +\"Correct the text\"\n",
    "\n",
    "boros_complex  =\"{content_html}\"+f\"\"\" \\n \\n \"\"\" + f\"\"\"Please assist with reviewing and correcting errors in texts produced by automatic transcription (OCR) of historical documents.\n",
    "Your task is to carefully examine the following text and correct any mistakes introduced by the OCR software. \n",
    "Do not write anything else than the corrected text.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_alt_endpoint = {'alt_endpoint':{'base_url':'https://api.groq.com/openai/v1',\n",
    "                     'api_key':os.getenv(\"GROQ_API_KEY\")}}\n",
    "\n",
    "basic_model_configs = pd.DataFrame({\n",
    "    'get_response_func': [get_response_openai, get_response_openai, get_response_anthropic, get_response_anthropic, \n",
    "                          get_response_openai,# get_response_openai, \n",
    "                          get_response_openai, get_response_replicate,# get_response_replicate, \n",
    "                          get_response_replicate], \n",
    "    'engine': ['gpt-3.5-turbo', 'gpt-4-turbo-preview', \"claude-3-haiku-20240307\", \"claude-3-opus-20240229\", \n",
    "               'mixtral-8x7b-32768',#'llama2-70b-4096', \n",
    "               'gemma-7b-it', 'meta/meta-llama-3-70b-instruct', 'meta/meta-llama-3-70b',\n",
    "                # 'meta/llama-2-70b'\n",
    "                 ],\n",
    "    'rate_limit':[160e3, 80e3, 100e3, 40e3, 9e3, #15e3,\n",
    "                   15e3, 100e3,# 100e3,\n",
    "                     100e3],\n",
    "    'additional_args': [\n",
    "        {}, {}, {}, {}, \n",
    "        groq_alt_endpoint, \n",
    "        #groq_alt_endpoint, \n",
    "        groq_alt_endpoint,\n",
    "        {}, {}#,{}\n",
    "    ]\n",
    "})\n",
    "\n",
    "full_model_configs = generate_model_configs(basic_model_configs, full_prompt, 'full')\n",
    "instruct_model_configs = generate_model_configs(basic_model_configs, instruct_prompt, 'instruct')\n",
    "\n",
    "#I think on reflection I only need to compare boros complex on gpt-4 as this was the best performer in their paper\n",
    "boros_configs_ncse = [\n",
    "    (get_response_openai, 'gpt-4-turbo-preview', boros_complex, \"boros_complex_\"),\n",
    "]\n",
    "\n",
    "boros_list = [\n",
    "    create_config_dict_func(\n",
    "        get_response_func=config[0],\n",
    "        rate_limiter=RateLimiter(80e3),\n",
    "        engine=config[1],\n",
    "        system_message_template=\"\",\n",
    "        prompt_template=config[2],\n",
    "        additional_args={\"response_name\": config[3]}\n",
    "    )\n",
    "    for config in boros_configs_ncse\n",
    "]\n",
    "\n",
    "model_configs = full_model_configs + instruct_model_configs + boros_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform all API calls\n",
    "\n",
    "The below section is what actually calls the API, the code points to the folders where the raw OCR is and provides a path to where the corrected text should be stored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NCSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_folder = ncse_articles_results\n",
    "#\n",
    "# This naming business needs to be cleaned up so the actual article ID is used. until then just have the following mess\n",
    "#\n",
    "# \n",
    "\n",
    "test_data_new = pd.read_csv(os.path.join(dev_data_folder,'transcription_raw_ocr.csv'))\n",
    "test_data_new = test_data_new.loc[test_data_new ['file_name'].isin(files_to_df_func(ncse_articles_transcribed )['file_name'])] #subset to just the data I have transcribed\n",
    "\n",
    "#This goes through the list of articles that have been transcribed, checks to see if there is a corrected version and if not generates it\n",
    "compare_request_configurations(test_data_new, model_configs, folder_path=corrected_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boros etal re-evaluation\n",
    "\n",
    "The post-OCR correction worked so well that the Boros etal prompt is being re-evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "boros_complex  =\"{content}\"+f\"\"\" \\n \\n \"\"\" + f\"\"\"Please assist with reviewing and correcting errors in texts produced by automatic transcription (OCR) of historical documents.\n",
    "Your task is to carefully examine the following text and correct any mistakes introduced by the OCR software. \n",
    "Do not write anything else than the corrected text.\"\"\"\n",
    "\n",
    "\n",
    "boros_config = generate_model_configs(basic_model_configs.iloc[0:2, :], boros_complex, 'boros_complex')[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sydney Morning Herald\n",
    "\n",
    "This section performs the correction test on dataset 2 of the Overproof collection. This is data from the Sydney Morning Herald. In addition it re-tests the Boros et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "smh_data = files_to_df_core_func(smh_articles_raw )\n",
    "\n",
    "smh_data['content'] = smh_data['content'].str.replace('\\n', ' ')\n",
    "\n",
    "smh_data['id'] = smh_data['file_name'] # this is needed as the processing log uses an id to keep track of what has been processed and what hasn't to allow for easy restarts\n",
    "\n",
    "full_prompt_smh = \"{content}\"+f\"\"\" \\n \\n \"\"\" + f\"\"\"You are an expert in post-OCR correction of documents. Using the context available from the text please recover the most likely original text from the corrupted OCR. The text is from The Sydney Morning Herald 1842 -1950. The text may be an advert or article and may be missing the beggining or end. Do not add any text, commentary, or lead in sentences beyond the recovered text. Do not add a title, or any introductions.\"\"\"\n",
    "\n",
    "instruct_prompt_smh = \"{content}\"+f\"\"\" \\n \\n \"\"\" + f\"\"\"You are an expert in post-OCR correction of documents. Using the context available from the text please recover the most likely original text from the corrupted OCR. Do not add any text, commentary, or lead in sentences beyond the recovered text. Do not add a title, or any introductions.\"\"\"\n",
    "\n",
    "full_model_configs_smh = generate_model_configs(basic_model_configs, full_prompt_smh, 'full')\n",
    "instruct_model_configs_smh = generate_model_configs(basic_model_configs, instruct_prompt_smh, 'instruct')\n",
    "\n",
    "#Boros et al prompt added in as the overall system works so well, it seems strange theirs didn't work, this is a quick check\n",
    "smh_configs = full_model_configs_smh + instruct_model_configs_smh  + [boros_config ]\n",
    "\n",
    "corrected_folder_smh = smh_articles_results\n",
    "\n",
    "compare_request_configurations(smh_data, smh_configs, folder_path=corrected_folder_smh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chronicalling America\n",
    "\n",
    "This section performs the correction test on dataset 2 of the Overproof collection. This is data from the Chronicalling America Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_data = files_to_df_core_func(ca_articles_raw )\n",
    "\n",
    "ca_data['content'] = ca_data['content'].str.replace('\\n', ' ')\n",
    "\n",
    "ca_data['id'] = ca_data['file_name'] # this is needed as the processing log uses an id to keep track of what has been processed and what hasn't to allow for easy restarts\n",
    "\n",
    "full_prompt_ca = \"{content}\"+f\"\"\" \\n \\n \"\"\" + f\"\"\"You are an expert in post-OCR correction of documents. Using the context available from the text please recover the most likely original text from the corrupted OCR. The text is from American Newspapers 1870 -1922. The text may be an advert or article and may be missing the beggining or end. Do not add any text, commentary, or lead in sentences beyond the recovered text. Do not add a title, or any introductions.\"\"\"\n",
    "\n",
    "instruct_prompt_ca = \"{content}\"+f\"\"\" \\n \\n \"\"\" + f\"\"\"You are an expert in post-OCR correction of documents. Using the context available from the text please recover the most likely original text from the corrupted OCR. Do not add any text, commentary, or lead in sentences beyond the recovered text. Do not add a title, or any introductions.\"\"\"\n",
    "\n",
    "\n",
    "full_model_configs_ca = generate_model_configs(basic_model_configs, full_prompt_ca, 'full')\n",
    "instruct_model_configs_ca = generate_model_configs(basic_model_configs, instruct_prompt_ca, 'instruct')\n",
    "\n",
    "ca_configs = full_model_configs_ca + instruct_model_configs_ca  + [boros_config ]\n",
    "\n",
    "corrected_folder_ca = ca_articles_results\n",
    "\n",
    "compare_request_configurations(ca_data, ca_configs, folder_path=corrected_folder_ca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the prompts across all models\n",
    "\n",
    "On the smaller models, Full is worse than instruct on the larger models the reverse. Maybe this is related to ability to 'focus' or hold isntructions in memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>WER</th>\n",
       "      <th>CER</th>\n",
       "      <th>lev_dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b</th>\n",
       "      <td>-102.41</td>\n",
       "      <td>-587.99</td>\n",
       "      <td>-592.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b</th>\n",
       "      <td>-102.68</td>\n",
       "      <td>-481.02</td>\n",
       "      <td>-468.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__llama2-70b-4096</th>\n",
       "      <td>56.83</td>\n",
       "      <td>-28.27</td>\n",
       "      <td>-27.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama2-70b-4096</th>\n",
       "      <td>53.24</td>\n",
       "      <td>-11.20</td>\n",
       "      <td>-12.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gemma-7b-it</th>\n",
       "      <td>20.47</td>\n",
       "      <td>-2.34</td>\n",
       "      <td>-2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gemma-7b-it</th>\n",
       "      <td>20.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__mixtral-8x7b-32768</th>\n",
       "      <td>48.72</td>\n",
       "      <td>6.60</td>\n",
       "      <td>5.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__mixtral-8x7b-32768</th>\n",
       "      <td>48.48</td>\n",
       "      <td>7.01</td>\n",
       "      <td>6.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-haiku-20240307</th>\n",
       "      <td>46.59</td>\n",
       "      <td>14.04</td>\n",
       "      <td>13.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b-instruct</th>\n",
       "      <td>46.43</td>\n",
       "      <td>16.15</td>\n",
       "      <td>14.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b-instruct</th>\n",
       "      <td>48.06</td>\n",
       "      <td>19.12</td>\n",
       "      <td>17.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-haiku-20240307</th>\n",
       "      <td>55.64</td>\n",
       "      <td>27.80</td>\n",
       "      <td>27.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-3.5-turbo</th>\n",
       "      <td>62.41</td>\n",
       "      <td>37.65</td>\n",
       "      <td>36.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-3.5-turbo</th>\n",
       "      <td>67.98</td>\n",
       "      <td>39.38</td>\n",
       "      <td>38.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-4-turbo-preview</th>\n",
       "      <td>77.27</td>\n",
       "      <td>59.82</td>\n",
       "      <td>57.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-4-turbo-preview</th>\n",
       "      <td>77.78</td>\n",
       "      <td>60.42</td>\n",
       "      <td>56.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boros_complex__gpt-4-turbo-preview</th>\n",
       "      <td>77.27</td>\n",
       "      <td>61.67</td>\n",
       "      <td>60.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-opus-20240229</th>\n",
       "      <td>78.39</td>\n",
       "      <td>62.71</td>\n",
       "      <td>61.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-opus-20240229</th>\n",
       "      <td>80.22</td>\n",
       "      <td>64.09</td>\n",
       "      <td>62.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        WER     CER lev_dist\n",
       "                                        50%     50%      50%\n",
       "type                                                        \n",
       "instruct__meta-llama-3-70b          -102.41 -587.99  -592.88\n",
       "full__meta-llama-3-70b              -102.68 -481.02  -468.17\n",
       "full__llama2-70b-4096                 56.83  -28.27   -27.33\n",
       "instruct__llama2-70b-4096             53.24  -11.20   -12.44\n",
       "full__gemma-7b-it                     20.47   -2.34    -2.55\n",
       "instruct__gemma-7b-it                 20.35    0.15     0.12\n",
       "full__mixtral-8x7b-32768              48.72    6.60     5.74\n",
       "instruct__mixtral-8x7b-32768          48.48    7.01     6.69\n",
       "full__claude-3-haiku-20240307         46.59   14.04    13.70\n",
       "instruct__meta-llama-3-70b-instruct   46.43   16.15    14.76\n",
       "full__meta-llama-3-70b-instruct       48.06   19.12    17.32\n",
       "instruct__claude-3-haiku-20240307     55.64   27.80    27.46\n",
       "full__gpt-3.5-turbo                   62.41   37.65    36.28\n",
       "instruct__gpt-3.5-turbo               67.98   39.38    38.48\n",
       "instruct__gpt-4-turbo-preview         77.27   59.82    57.58\n",
       "full__gpt-4-turbo-preview             77.78   60.42    56.37\n",
       "boros_complex__gpt-4-turbo-preview    77.27   61.67    60.02\n",
       "instruct__claude-3-opus-20240229      78.39   62.71    61.62\n",
       "full__claude-3-opus-20240229          80.22   64.09    62.78"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "## double check get_metric_error_reduction\n",
    "##\n",
    "\n",
    "corrected_folder = ncse_articles_results \n",
    "\n",
    "gt_folder = ncse_articles_transcribed \n",
    "\n",
    "raw_ocr = ncse_articles_raw\n",
    "\n",
    "ncse_performance_eval =  evaluate_correction_performance_folders(corrected_folder, gt_folder, wer, cer)\n",
    "\n",
    "ncse_raw_ocr_eval =  evaluate_correction_performance(raw_ocr, gt_folder , wer, cer, 'raw_ocr')\n",
    "\n",
    "\n",
    "ncse_error_reduction = get_metric_error_reduction(ncse_performance_eval, ncse_raw_ocr_eval )\n",
    "\n",
    "ncse_error_reduction.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          WER   CER lev_dist\n",
      "          50%   50%      50%\n",
      "type                        \n",
      "raw_ocr  0.63  0.17    341.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>WER</th>\n",
       "      <th>CER</th>\n",
       "      <th>lev_dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-opus-20240229</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.06</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-opus-20240229</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.07</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boros_complex__gpt-4-turbo-preview</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>127.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-4-turbo-preview</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.09</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-4-turbo-preview</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.10</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-3.5-turbo</th>\n",
       "      <td>0.28</td>\n",
       "      <td>0.13</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__llama2-70b-4096</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama2-70b-4096</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.15</td>\n",
       "      <td>259.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-3.5-turbo</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.23</td>\n",
       "      <td>197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__mixtral-8x7b-32768</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.32</td>\n",
       "      <td>337.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-haiku-20240307</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.36</td>\n",
       "      <td>272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__mixtral-8x7b-32768</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.36</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b-instruct</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.39</td>\n",
       "      <td>577.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b-instruct</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.40</td>\n",
       "      <td>492.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-haiku-20240307</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.44</td>\n",
       "      <td>347.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gemma-7b-it</th>\n",
       "      <td>0.66</td>\n",
       "      <td>0.52</td>\n",
       "      <td>501.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gemma-7b-it</th>\n",
       "      <td>0.69</td>\n",
       "      <td>0.52</td>\n",
       "      <td>614.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1832.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1843.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      WER   CER lev_dist\n",
       "                                      50%   50%      50%\n",
       "type                                                    \n",
       "instruct__claude-3-opus-20240229     0.15  0.06    115.0\n",
       "full__claude-3-opus-20240229         0.15  0.07    113.0\n",
       "boros_complex__gpt-4-turbo-preview   0.17  0.09    127.0\n",
       "full__gpt-4-turbo-preview            0.17  0.09    114.0\n",
       "instruct__gpt-4-turbo-preview        0.17  0.10    126.0\n",
       "instruct__gpt-3.5-turbo              0.28  0.13    172.0\n",
       "full__llama2-70b-4096                0.21  0.14    317.0\n",
       "instruct__llama2-70b-4096            0.22  0.15    259.5\n",
       "full__gpt-3.5-turbo                  0.36  0.23    197.0\n",
       "instruct__mixtral-8x7b-32768         0.46  0.32    337.0\n",
       "instruct__claude-3-haiku-20240307    0.46  0.36    272.0\n",
       "full__mixtral-8x7b-32768             0.51  0.36    394.0\n",
       "instruct__meta-llama-3-70b-instruct  0.52  0.39    577.0\n",
       "full__meta-llama-3-70b-instruct      0.52  0.40    492.0\n",
       "full__claude-3-haiku-20240307        0.55  0.44    347.0\n",
       "instruct__gemma-7b-it                0.66  0.52    501.0\n",
       "full__gemma-7b-it                    0.69  0.52    614.0\n",
       "full__meta-llama-3-70b               0.98  0.91   1832.0\n",
       "instruct__meta-llama-3-70b           0.99  0.94   1843.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ncse_raw_ocr_eval.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%')))\n",
    "ncse_performance_eval.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>WER</th>\n",
       "      <th>CER</th>\n",
       "      <th>lev_dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>full__llama-2-70b</th>\n",
       "      <td>-89.25</td>\n",
       "      <td>-963.01</td>\n",
       "      <td>-914.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama-2-70b</th>\n",
       "      <td>-89.50</td>\n",
       "      <td>-958.60</td>\n",
       "      <td>-895.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b</th>\n",
       "      <td>-88.52</td>\n",
       "      <td>-867.47</td>\n",
       "      <td>-865.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b</th>\n",
       "      <td>-90.56</td>\n",
       "      <td>-864.63</td>\n",
       "      <td>-853.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gemma-7b-it</th>\n",
       "      <td>3.26</td>\n",
       "      <td>-35.65</td>\n",
       "      <td>-35.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__mixtral-8x7b-32768</th>\n",
       "      <td>17.39</td>\n",
       "      <td>-19.11</td>\n",
       "      <td>-20.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__mixtral-8x7b-32768</th>\n",
       "      <td>18.00</td>\n",
       "      <td>-14.63</td>\n",
       "      <td>-15.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gemma-7b-it</th>\n",
       "      <td>9.66</td>\n",
       "      <td>-12.93</td>\n",
       "      <td>-13.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__llama2-70b-4096</th>\n",
       "      <td>16.67</td>\n",
       "      <td>5.77</td>\n",
       "      <td>1.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama2-70b-4096</th>\n",
       "      <td>16.00</td>\n",
       "      <td>6.45</td>\n",
       "      <td>3.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b-instruct</th>\n",
       "      <td>18.75</td>\n",
       "      <td>12.90</td>\n",
       "      <td>8.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b-instruct</th>\n",
       "      <td>20.56</td>\n",
       "      <td>17.95</td>\n",
       "      <td>13.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overproof</th>\n",
       "      <td>19.23</td>\n",
       "      <td>28.38</td>\n",
       "      <td>27.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-haiku-20240307</th>\n",
       "      <td>26.92</td>\n",
       "      <td>35.71</td>\n",
       "      <td>33.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-haiku-20240307</th>\n",
       "      <td>27.34</td>\n",
       "      <td>38.38</td>\n",
       "      <td>37.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-3.5-turbo</th>\n",
       "      <td>26.72</td>\n",
       "      <td>39.18</td>\n",
       "      <td>38.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-4-turbo-preview</th>\n",
       "      <td>30.20</td>\n",
       "      <td>41.79</td>\n",
       "      <td>41.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-4-turbo-preview</th>\n",
       "      <td>30.00</td>\n",
       "      <td>42.08</td>\n",
       "      <td>41.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-3.5-turbo</th>\n",
       "      <td>27.75</td>\n",
       "      <td>42.86</td>\n",
       "      <td>41.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-opus-20240229</th>\n",
       "      <td>32.72</td>\n",
       "      <td>45.45</td>\n",
       "      <td>41.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boros_complex__gpt-4-turbo-preview</th>\n",
       "      <td>30.71</td>\n",
       "      <td>48.44</td>\n",
       "      <td>48.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-opus-20240229</th>\n",
       "      <td>32.11</td>\n",
       "      <td>51.03</td>\n",
       "      <td>48.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       WER     CER lev_dist\n",
       "                                       50%     50%      50%\n",
       "type                                                       \n",
       "full__llama-2-70b                   -89.25 -963.01  -914.86\n",
       "instruct__llama-2-70b               -89.50 -958.60  -895.77\n",
       "instruct__meta-llama-3-70b          -88.52 -867.47  -865.27\n",
       "full__meta-llama-3-70b              -90.56 -864.63  -853.01\n",
       "full__gemma-7b-it                     3.26  -35.65   -35.34\n",
       "instruct__mixtral-8x7b-32768         17.39  -19.11   -20.00\n",
       "full__mixtral-8x7b-32768             18.00  -14.63   -15.24\n",
       "instruct__gemma-7b-it                 9.66  -12.93   -13.04\n",
       "full__llama2-70b-4096                16.67    5.77     1.85\n",
       "instruct__llama2-70b-4096            16.00    6.45     3.08\n",
       "full__meta-llama-3-70b-instruct      18.75   12.90     8.40\n",
       "instruct__meta-llama-3-70b-instruct  20.56   17.95    13.33\n",
       "overproof                            19.23   28.38    27.59\n",
       "instruct__claude-3-haiku-20240307    26.92   35.71    33.80\n",
       "full__claude-3-haiku-20240307        27.34   38.38    37.48\n",
       "full__gpt-3.5-turbo                  26.72   39.18    38.66\n",
       "instruct__gpt-4-turbo-preview        30.20   41.79    41.18\n",
       "full__gpt-4-turbo-preview            30.00   42.08    41.46\n",
       "instruct__gpt-3.5-turbo              27.75   42.86    41.46\n",
       "instruct__claude-3-opus-20240229     32.72   45.45    41.57\n",
       "boros_complex__gpt-4-turbo-preview   30.71   48.44    48.23\n",
       "full__claude-3-opus-20240229         32.11   51.03    48.78"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_folder = smh_articles_results \n",
    "\n",
    "gt_folder = smh_articles_transcribed \n",
    "\n",
    "raw_ocr = smh_articles_raw\n",
    "\n",
    "smh_performance_eval =  evaluate_correction_performance_folders(corrected_folder, gt_folder, wer, cer)\n",
    "\n",
    "smh_raw_ocr_eval =  evaluate_correction_performance(raw_ocr, gt_folder , wer, cer, 'raw_ocr')\n",
    "\n",
    "smh_error_reduction = get_metric_error_reduction(smh_performance_eval, smh_raw_ocr_eval )\n",
    "\n",
    "smh_error_reduction.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          WER   CER lev_dist\n",
      "          50%   50%      50%\n",
      "type                        \n",
      "raw_ocr  0.51  0.08    131.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>WER</th>\n",
       "      <th>CER</th>\n",
       "      <th>lev_dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>boros_complex__gpt-4-turbo-preview</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.04</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-haiku-20240307</th>\n",
       "      <td>0.37</td>\n",
       "      <td>0.04</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-opus-20240229</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.04</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-3.5-turbo</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.04</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-3.5-turbo</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.04</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-opus-20240229</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.04</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-4-turbo-preview</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-haiku-20240307</th>\n",
       "      <td>0.36</td>\n",
       "      <td>0.05</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overproof</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.05</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-4-turbo-preview</th>\n",
       "      <td>0.34</td>\n",
       "      <td>0.05</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b-instruct</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__llama2-70b-4096</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>115.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama2-70b-4096</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b-instruct</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.07</td>\n",
       "      <td>83.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__mixtral-8x7b-32768</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.10</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__mixtral-8x7b-32768</th>\n",
       "      <td>0.41</td>\n",
       "      <td>0.10</td>\n",
       "      <td>147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gemma-7b-it</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.12</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gemma-7b-it</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.17</td>\n",
       "      <td>212.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.84</td>\n",
       "      <td>1307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.87</td>\n",
       "      <td>1343.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__llama-2-70b</th>\n",
       "      <td>0.97</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama-2-70b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1324.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      WER   CER lev_dist\n",
       "                                      50%   50%      50%\n",
       "type                                                    \n",
       "boros_complex__gpt-4-turbo-preview   0.34  0.04     65.0\n",
       "full__claude-3-haiku-20240307        0.37  0.04     73.0\n",
       "full__claude-3-opus-20240229         0.34  0.04     69.0\n",
       "full__gpt-3.5-turbo                  0.35  0.04     87.0\n",
       "instruct__gpt-3.5-turbo              0.35  0.04     78.0\n",
       "instruct__claude-3-opus-20240229     0.34  0.04     71.0\n",
       "instruct__gpt-4-turbo-preview        0.34  0.05     75.0\n",
       "instruct__claude-3-haiku-20240307    0.36  0.05     81.0\n",
       "overproof                            0.40  0.05     88.0\n",
       "full__gpt-4-turbo-preview            0.34  0.05     74.0\n",
       "full__meta-llama-3-70b-instruct      0.42  0.07    100.0\n",
       "full__llama2-70b-4096                0.40  0.07    115.0\n",
       "instruct__llama2-70b-4096            0.40  0.07    117.0\n",
       "instruct__meta-llama-3-70b-instruct  0.41  0.07     83.0\n",
       "instruct__mixtral-8x7b-32768         0.40  0.10    159.0\n",
       "full__mixtral-8x7b-32768             0.41  0.10    147.0\n",
       "instruct__gemma-7b-it                0.48  0.12    160.0\n",
       "full__gemma-7b-it                    0.51  0.17    212.0\n",
       "full__meta-llama-3-70b               0.97  0.84   1307.0\n",
       "instruct__meta-llama-3-70b           0.97  0.87   1343.0\n",
       "full__llama-2-70b                    0.97  0.89   1339.0\n",
       "instruct__llama-2-70b                0.98  0.89   1324.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(smh_raw_ocr_eval.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%')))\n",
    "\n",
    "smh_performance_eval.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>WER</th>\n",
       "      <th>CER</th>\n",
       "      <th>lev_dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>full__llama-2-70b</th>\n",
       "      <td>-59.57</td>\n",
       "      <td>-784.90</td>\n",
       "      <td>-761.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama-2-70b</th>\n",
       "      <td>-61.06</td>\n",
       "      <td>-752.31</td>\n",
       "      <td>-741.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b</th>\n",
       "      <td>-71.75</td>\n",
       "      <td>-741.39</td>\n",
       "      <td>-739.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b</th>\n",
       "      <td>-63.14</td>\n",
       "      <td>-729.38</td>\n",
       "      <td>-725.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gemma-7b-it</th>\n",
       "      <td>5.66</td>\n",
       "      <td>-41.00</td>\n",
       "      <td>-42.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gemma-7b-it</th>\n",
       "      <td>3.04</td>\n",
       "      <td>-38.01</td>\n",
       "      <td>-38.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__mixtral-8x7b-32768</th>\n",
       "      <td>13.72</td>\n",
       "      <td>-22.07</td>\n",
       "      <td>-22.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__mixtral-8x7b-32768</th>\n",
       "      <td>15.72</td>\n",
       "      <td>-16.30</td>\n",
       "      <td>-14.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b-instruct</th>\n",
       "      <td>9.74</td>\n",
       "      <td>-9.42</td>\n",
       "      <td>-10.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama2-70b-4096</th>\n",
       "      <td>10.86</td>\n",
       "      <td>-6.48</td>\n",
       "      <td>-7.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__llama2-70b-4096</th>\n",
       "      <td>10.19</td>\n",
       "      <td>-4.90</td>\n",
       "      <td>-8.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b-instruct</th>\n",
       "      <td>12.91</td>\n",
       "      <td>14.24</td>\n",
       "      <td>10.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-haiku-20240307</th>\n",
       "      <td>14.97</td>\n",
       "      <td>26.01</td>\n",
       "      <td>25.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overproof</th>\n",
       "      <td>21.63</td>\n",
       "      <td>34.59</td>\n",
       "      <td>34.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-haiku-20240307</th>\n",
       "      <td>17.41</td>\n",
       "      <td>34.73</td>\n",
       "      <td>34.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-4-turbo-preview</th>\n",
       "      <td>20.98</td>\n",
       "      <td>37.57</td>\n",
       "      <td>37.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-4-turbo-preview</th>\n",
       "      <td>21.84</td>\n",
       "      <td>38.18</td>\n",
       "      <td>37.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-3.5-turbo</th>\n",
       "      <td>24.62</td>\n",
       "      <td>44.05</td>\n",
       "      <td>44.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-3.5-turbo</th>\n",
       "      <td>22.90</td>\n",
       "      <td>44.22</td>\n",
       "      <td>43.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boros_complex__gpt-4-turbo-preview</th>\n",
       "      <td>24.02</td>\n",
       "      <td>45.55</td>\n",
       "      <td>45.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-opus-20240229</th>\n",
       "      <td>24.26</td>\n",
       "      <td>47.01</td>\n",
       "      <td>41.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-opus-20240229</th>\n",
       "      <td>24.61</td>\n",
       "      <td>48.23</td>\n",
       "      <td>43.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       WER     CER lev_dist\n",
       "                                       50%     50%      50%\n",
       "type                                                       \n",
       "full__llama-2-70b                   -59.57 -784.90  -761.72\n",
       "instruct__llama-2-70b               -61.06 -752.31  -741.10\n",
       "full__meta-llama-3-70b              -71.75 -741.39  -739.31\n",
       "instruct__meta-llama-3-70b          -63.14 -729.38  -725.21\n",
       "instruct__gemma-7b-it                 5.66  -41.00   -42.38\n",
       "full__gemma-7b-it                     3.04  -38.01   -38.15\n",
       "instruct__mixtral-8x7b-32768         13.72  -22.07   -22.36\n",
       "full__mixtral-8x7b-32768             15.72  -16.30   -14.86\n",
       "full__meta-llama-3-70b-instruct       9.74   -9.42   -10.51\n",
       "instruct__llama2-70b-4096            10.86   -6.48    -7.94\n",
       "full__llama2-70b-4096                10.19   -4.90    -8.14\n",
       "instruct__meta-llama-3-70b-instruct  12.91   14.24    10.90\n",
       "full__claude-3-haiku-20240307        14.97   26.01    25.27\n",
       "overproof                            21.63   34.59    34.59\n",
       "instruct__claude-3-haiku-20240307    17.41   34.73    34.73\n",
       "instruct__gpt-4-turbo-preview        20.98   37.57    37.45\n",
       "full__gpt-4-turbo-preview            21.84   38.18    37.70\n",
       "instruct__gpt-3.5-turbo              24.62   44.05    44.05\n",
       "full__gpt-3.5-turbo                  22.90   44.22    43.99\n",
       "boros_complex__gpt-4-turbo-preview   24.02   45.55    45.19\n",
       "instruct__claude-3-opus-20240229     24.26   47.01    41.18\n",
       "full__claude-3-opus-20240229         24.61   48.23    43.44"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_folder = ca_articles_results \n",
    "\n",
    "gt_folder = ca_articles_transcribed \n",
    "\n",
    "raw_ocr = ca_articles_raw\n",
    "\n",
    "ca_performance_eval =  evaluate_correction_performance_folders(corrected_folder, gt_folder, wer, cer)\n",
    "\n",
    "ca_raw_ocr_eval =  evaluate_correction_performance(raw_ocr, gt_folder , wer, cer, 'raw_ocr')\n",
    "\n",
    "ca_error_reduction = get_metric_error_reduction(ca_performance_eval, ca_raw_ocr_eval )\n",
    "\n",
    "ca_error_reduction.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          WER  CER lev_dist\n",
      "          50%  50%      50%\n",
      "type                       \n",
      "raw_ocr  0.62  0.1    198.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>WER</th>\n",
       "      <th>CER</th>\n",
       "      <th>lev_dist</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>boros_complex__gpt-4-turbo-preview</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.05</td>\n",
       "      <td>107.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-opus-20240229</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.05</td>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-3.5-turbo</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.05</td>\n",
       "      <td>100.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-opus-20240229</th>\n",
       "      <td>0.44</td>\n",
       "      <td>0.05</td>\n",
       "      <td>119.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gpt-4-turbo-preview</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.06</td>\n",
       "      <td>118.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-3.5-turbo</th>\n",
       "      <td>0.46</td>\n",
       "      <td>0.06</td>\n",
       "      <td>108.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gpt-4-turbo-preview</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.06</td>\n",
       "      <td>120.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__claude-3-haiku-20240307</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.06</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>overproof</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.07</td>\n",
       "      <td>111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__claude-3-haiku-20240307</th>\n",
       "      <td>0.48</td>\n",
       "      <td>0.07</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b-instruct</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.10</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b-instruct</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.11</td>\n",
       "      <td>145.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__llama2-70b-4096</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.11</td>\n",
       "      <td>203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama2-70b-4096</th>\n",
       "      <td>0.51</td>\n",
       "      <td>0.11</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__mixtral-8x7b-32768</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>234.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__mixtral-8x7b-32768</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.12</td>\n",
       "      <td>230.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__gemma-7b-it</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.15</td>\n",
       "      <td>239.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__gemma-7b-it</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.15</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__meta-llama-3-70b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1784.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__meta-llama-3-70b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.89</td>\n",
       "      <td>1696.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instruct__llama-2-70b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.91</td>\n",
       "      <td>1608.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>full__llama-2-70b</th>\n",
       "      <td>0.98</td>\n",
       "      <td>0.92</td>\n",
       "      <td>1589.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      WER   CER lev_dist\n",
       "                                      50%   50%      50%\n",
       "type                                                    \n",
       "boros_complex__gpt-4-turbo-preview   0.45  0.05    107.0\n",
       "full__claude-3-opus-20240229         0.43  0.05    117.0\n",
       "instruct__gpt-3.5-turbo              0.45  0.05    100.5\n",
       "instruct__claude-3-opus-20240229     0.44  0.05    119.0\n",
       "instruct__gpt-4-turbo-preview        0.45  0.06    118.0\n",
       "full__gpt-3.5-turbo                  0.46  0.06    108.0\n",
       "full__gpt-4-turbo-preview            0.45  0.06    120.5\n",
       "instruct__claude-3-haiku-20240307    0.47  0.06    112.0\n",
       "overproof                            0.47  0.07    111.0\n",
       "full__claude-3-haiku-20240307        0.48  0.07    123.0\n",
       "instruct__meta-llama-3-70b-instruct  0.54  0.10    132.0\n",
       "full__meta-llama-3-70b-instruct      0.55  0.11    145.5\n",
       "full__llama2-70b-4096                0.50  0.11    203.0\n",
       "instruct__llama2-70b-4096            0.51  0.11    195.0\n",
       "instruct__mixtral-8x7b-32768         0.50  0.12    234.5\n",
       "full__mixtral-8x7b-32768             0.50  0.12    230.5\n",
       "instruct__gemma-7b-it                0.57  0.15    239.5\n",
       "full__gemma-7b-it                    0.59  0.15    299.0\n",
       "full__meta-llama-3-70b               0.98  0.86   1784.5\n",
       "instruct__meta-llama-3-70b           0.98  0.89   1696.0\n",
       "instruct__llama-2-70b                0.98  0.91   1608.5\n",
       "full__llama-2-70b                    0.98  0.92   1589.5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(ca_raw_ocr_eval.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%')))\n",
    "\n",
    "ca_performance_eval.groupby('type').describe().filter(regex = '50|median').round(2).sort_values((eval_metric, '50%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>prompt</th>\n",
       "      <th>NCSE</th>\n",
       "      <th>SMH</th>\n",
       "      <th>CA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Llama 2 70B</td>\n",
       "      <td>full</td>\n",
       "      <td>-28.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>-4.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama 2 70B</td>\n",
       "      <td>instruct</td>\n",
       "      <td>-11.2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gemma 7B</td>\n",
       "      <td>full</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-35.7</td>\n",
       "      <td>-38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemma 7B</td>\n",
       "      <td>instruct</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>-41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mixtral 8x7B</td>\n",
       "      <td>full</td>\n",
       "      <td>6.6</td>\n",
       "      <td>-14.6</td>\n",
       "      <td>-16.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mixtral 8x7B</td>\n",
       "      <td>instruct</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-19.1</td>\n",
       "      <td>-22.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Claude 3 Haiku</td>\n",
       "      <td>full</td>\n",
       "      <td>14.0</td>\n",
       "      <td>38.4</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>instruct</td>\n",
       "      <td>16.1</td>\n",
       "      <td>17.9</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>full</td>\n",
       "      <td>19.1</td>\n",
       "      <td>12.9</td>\n",
       "      <td>-9.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Claude 3 Haiku</td>\n",
       "      <td>instruct</td>\n",
       "      <td>27.8</td>\n",
       "      <td>35.7</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>full</td>\n",
       "      <td>37.7</td>\n",
       "      <td>39.2</td>\n",
       "      <td>44.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>instruct</td>\n",
       "      <td>39.4</td>\n",
       "      <td>42.9</td>\n",
       "      <td>44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>instruct</td>\n",
       "      <td>59.8</td>\n",
       "      <td>41.8</td>\n",
       "      <td>37.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>full</td>\n",
       "      <td>60.4</td>\n",
       "      <td>42.1</td>\n",
       "      <td>38.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GPT-4 Boros</td>\n",
       "      <td>boros</td>\n",
       "      <td>61.7</td>\n",
       "      <td>48.4</td>\n",
       "      <td>45.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Claude 3 Opus</td>\n",
       "      <td>instruct</td>\n",
       "      <td>62.7</td>\n",
       "      <td>45.5</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Claude 3 Opus</td>\n",
       "      <td>full</td>\n",
       "      <td>64.1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>48.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Overproof</td>\n",
       "      <td>overproof</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.4</td>\n",
       "      <td>34.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model     prompt  NCSE   SMH    CA\n",
       "0      Llama 2 70B       full -28.3   5.8  -4.9\n",
       "1      Llama 2 70B   instruct -11.2   6.5  -6.5\n",
       "2         Gemma 7B       full  -2.3 -35.7 -38.0\n",
       "3         Gemma 7B   instruct   0.1 -12.9 -41.0\n",
       "4     Mixtral 8x7B       full   6.6 -14.6 -16.3\n",
       "5     Mixtral 8x7B   instruct   7.0 -19.1 -22.1\n",
       "6   Claude 3 Haiku       full  14.0  38.4  26.0\n",
       "7          Llama 3   instruct  16.1  17.9  14.2\n",
       "8          Llama 3       full  19.1  12.9  -9.4\n",
       "9   Claude 3 Haiku   instruct  27.8  35.7  34.7\n",
       "10         GPT-3.5       full  37.7  39.2  44.2\n",
       "11         GPT-3.5   instruct  39.4  42.9  44.1\n",
       "12           GPT-4   instruct  59.8  41.8  37.6\n",
       "13           GPT-4       full  60.4  42.1  38.2\n",
       "14     GPT-4 Boros      boros  61.7  48.4  45.6\n",
       "15   Claude 3 Opus   instruct  62.7  45.5  47.0\n",
       "16   Claude 3 Opus       full  64.1  51.0  48.2\n",
       "17       Overproof  overproof   NaN  28.4  34.6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ncse_median = ncse_error_reduction.groupby('type')[eval_metric].median().round(1)\n",
    "smh_median = smh_error_reduction.groupby('type')[eval_metric].median().round(1)\n",
    "ca_median = ca_error_reduction.groupby('type')[eval_metric].median().round(1)\n",
    "\n",
    "# Combine the results into a new dataframe\n",
    "result_df = pd.DataFrame({\n",
    "    'NCSE': ncse_median,\n",
    "    'SMH': smh_median,\n",
    "    'CA': ca_median\n",
    "})\n",
    "\n",
    "# Reset the index to make 'type' a regular column\n",
    "result_df = result_df.reset_index()\n",
    "\n",
    "result_df = result_df.loc[~result_df['type'].isin(['boros_basic__gpt-4-turbo-preview',  'claude_temp_claude-3-opus-20240229'])]\n",
    "\n",
    "result_df['model'] = result_df['type'].str.split('_').str[-1]\n",
    "result_df['prompt'] = result_df['type'].str.split('_').str[0]\n",
    "\n",
    "result_df = result_df.sort_values('NCSE').merge(model_name_code.reset_index().rename(columns={0: 'model'}), on='model').rename(columns={'index': 'Model'})\n",
    "\n",
    "result_df['Model'] = np.where(result_df['prompt']=='boros', result_df['Model'] + \" Boros\", result_df['Model'])\n",
    "\n",
    "#result_df = result_df.loc[~(result_df['Model'].str.contains('Llama 2'))]\n",
    "\n",
    "result_df[['Model', 'prompt', 'NCSE', 'SMH', 'CA']].sort_values(['NCSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_latex_with_formatting(df, caption, label):\n",
    "\n",
    "    df = df.copy()\n",
    "    # Format the 'Total' row by appending \\textbf{} to each element\n",
    "    #df.iloc[-1] = df.iloc[-1].apply(lambda x: '\\\\textbf{' + str(x) + '}')\n",
    "    \n",
    "    # Convert DataFrame to LaTeX\n",
    "    latex_table = df.to_latex(\n",
    "        index=False,\n",
    "        float_format=\"%.2f\" ,\n",
    "        escape=False,  # Important to render LaTeX commands within the table properly\n",
    "        column_format='p{5cm}cccc',  # One left-aligned column followed by four centered columns\n",
    "        bold_rows=True,  # Bold the headers\n",
    "        caption=caption,\n",
    "        label=label\n",
    "    )\n",
    "    return latex_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Model performance across the datasets measured in Error Reduction Percentage, higher is better.There is significant variation in how well the LMs are able to perform post-OCR correction, and significant differences between prompts for certain models.}\n",
      "\\label{tab:results}\n",
      "\\begin{tabular}{p{5cm}cccc}\n",
      "\\toprule\n",
      "Model & NCSE & SMH & CA \\\\\n",
      "\\midrule\n",
      "Claude 3 Haiku & 14.00 & 38.40 & 26.00 \\\\\n",
      "Claude 3 Opus & 64.10 & 51.00 & 48.20 \\\\\n",
      "GPT-3.5 & 37.70 & 39.20 & 44.20 \\\\\n",
      "GPT-4 & 60.40 & 42.10 & 38.20 \\\\\n",
      "GPT-4 Boros & 61.70 & 48.40 & 45.60 \\\\\n",
      "Gemma 7B & -2.30 & -35.70 & -38.00 \\\\\n",
      "Llama 3 & 19.10 & 12.90 & -9.40 \\\\\n",
      "Mixtral 8x7B & 6.60 & -14.60 & -16.30 \\\\\n",
      "Overproof & NaN & 28.40 & 34.60 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_tab = result_df.loc[(result_df['prompt']!='instruct') & ~(result_df['Model'].str.contains('Llama 2')), ['Model',  'NCSE', 'SMH', 'CA']].sort_values(['Model'])\n",
    "\n",
    "results_tab = render_latex_with_formatting(results_tab,   \n",
    "                            'Model performance across the datasets measured in Error Reduction Percentage, higher is better.There is significant variation in how well the LMs are able to perform post-OCR correction, and significant differences between prompts for certain models.',\n",
    "                            'tab:results'  )\n",
    "\n",
    "print(results_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Model performance across the datasets measured in Error Reduction Percentage, higher is better.There is significant variation in how well the LMs are able to perform post-OCR correction, and significant differences between prompts for certain models.}\n",
      "\\label{tab:results}\n",
      "\\begin{tabular}{p{5cm}cccc}\n",
      "\\toprule\n",
      "Model & NCSE & SMH & CA \\\\\n",
      "\\midrule\n",
      "Claude 3 Haiku & 27.80 & 35.70 & 34.70 \\\\\n",
      "Claude 3 Opus & 62.70 & 45.50 & 47.00 \\\\\n",
      "GPT-3.5 & 39.40 & 42.90 & 44.10 \\\\\n",
      "GPT-4 & 59.80 & 41.80 & 37.60 \\\\\n",
      "Gemma 7B & 0.10 & -12.90 & -41.00 \\\\\n",
      "Llama 2 70B & -11.20 & 6.50 & -6.50 \\\\\n",
      "Llama 3 & 16.10 & 17.90 & 14.20 \\\\\n",
      "Mixtral 8x7B & 7.00 & -19.10 & -22.10 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_tab = render_latex_with_formatting(result_df.loc[result_df['prompt']=='instruct', ['Model',  'NCSE', 'SMH', 'CA']].sort_values(['Model']), \n",
    "                             'Model performance across the datasets measured in Error Reduction Percentage, higher is better.There is significant variation in how well the LMs are able to perform post-OCR correction, and significant differences between prompts for certain models.',\n",
    "                             'tab:results')\n",
    "\n",
    "print(results_tab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>NCSE</th>\n",
       "      <th>SMH</th>\n",
       "      <th>CA</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Claude 3 Haiku</td>\n",
       "      <td>27.8</td>\n",
       "      <td>35.7</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Claude 3 Opus</td>\n",
       "      <td>62.7</td>\n",
       "      <td>45.5</td>\n",
       "      <td>47.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>39.4</td>\n",
       "      <td>42.9</td>\n",
       "      <td>44.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>59.8</td>\n",
       "      <td>41.8</td>\n",
       "      <td>37.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Gemma 7B</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>-41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Llama 2 70B</td>\n",
       "      <td>-11.2</td>\n",
       "      <td>6.5</td>\n",
       "      <td>-6.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>16.1</td>\n",
       "      <td>17.9</td>\n",
       "      <td>14.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mixtral 8x7B</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-19.1</td>\n",
       "      <td>-22.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  NCSE   SMH    CA\n",
       "9   Claude 3 Haiku  27.8  35.7  34.7\n",
       "15   Claude 3 Opus  62.7  45.5  47.0\n",
       "11         GPT-3.5  39.4  42.9  44.1\n",
       "12           GPT-4  59.8  41.8  37.6\n",
       "3         Gemma 7B   0.1 -12.9 -41.0\n",
       "1      Llama 2 70B -11.2   6.5  -6.5\n",
       "7          Llama 3  16.1  17.9  14.2\n",
       "5     Mixtral 8x7B   7.0 -19.1 -22.1"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.loc[result_df['prompt']=='instruct', ['Model',  'NCSE', 'SMH', 'CA']].sort_values(['Model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_melted = result_df.melt(id_vars=['model', 'prompt'], value_vars=['NCSE', 'SMH', 'CA'], \n",
    "                    var_name='Dataset', value_name='Value')\n",
    "\n",
    "df_full = result_df[result_df['prompt'] == 'full']\n",
    "df_instruct = result_df[result_df['prompt'] == 'instruct']\n",
    "\n",
    "# Melt the filtered DataFrames\n",
    "df_full_melted = df_full.melt(id_vars=['model'], value_vars=['NCSE', 'SMH', 'CA'], var_name='Dataset', value_name='Value_Full')\n",
    "df_instruct_melted = df_instruct.melt(id_vars=['model'], value_vars=['NCSE', 'SMH', 'CA'], var_name='Dataset', value_name='Value_Instruct')\n",
    "\n",
    "df_merged = pd.merge(df_full_melted, df_instruct_melted, on=['model', 'Dataset'])\n",
    "\n",
    "# Calculate the difference\n",
    "df_merged['Difference'] = df_merged['Value_Full'] - df_merged['Value_Instruct']\n",
    "\n",
    "df_merged = df_merged.loc[~(df_merged['model'].str.contains('llama2')),:].merge(model_name_code.reset_index().rename(columns={0: 'model'}), on='model').rename(columns={'index': 'Model'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{The differnce between the two prompts across all models and datasets}\n",
      "\\label{tab:prompt_diff}\n",
      "\\begin{tabular}{p{5cm}cccc}\n",
      "\\toprule\n",
      "Model & Dataset & Value_Full & Value_Instruct & Difference \\\\\n",
      "\\midrule\n",
      "Claude 3 Haiku & NCSE & 14.00 & 27.80 & -13.80 \\\\\n",
      "Claude 3 Haiku & CA & 26.00 & 34.70 & -8.70 \\\\\n",
      "Claude 3 Haiku & SMH & 38.40 & 35.70 & 2.70 \\\\\n",
      "Claude 3 Opus & CA & 48.20 & 47.00 & 1.20 \\\\\n",
      "Claude 3 Opus & NCSE & 64.10 & 62.70 & 1.40 \\\\\n",
      "Claude 3 Opus & SMH & 51.00 & 45.50 & 5.50 \\\\\n",
      "GPT-3.5 & CA & 44.20 & 44.10 & 0.10 \\\\\n",
      "GPT-3.5 & NCSE & 37.70 & 39.40 & -1.70 \\\\\n",
      "GPT-3.5 & SMH & 39.20 & 42.90 & -3.70 \\\\\n",
      "GPT-4 & NCSE & 60.40 & 59.80 & 0.60 \\\\\n",
      "GPT-4 & CA & 38.20 & 37.60 & 0.60 \\\\\n",
      "GPT-4 & SMH & 42.10 & 41.80 & 0.30 \\\\\n",
      "Gemma 7B & CA & -38.00 & -41.00 & 3.00 \\\\\n",
      "Gemma 7B & NCSE & -2.30 & 0.10 & -2.40 \\\\\n",
      "Gemma 7B & SMH & -35.70 & -12.90 & -22.80 \\\\\n",
      "Llama 3 & NCSE & 19.10 & 16.10 & 3.00 \\\\\n",
      "Llama 3 & CA & -9.40 & 14.20 & -23.60 \\\\\n",
      "Llama 3 & SMH & 12.90 & 17.90 & -5.00 \\\\\n",
      "Mixtral 8x7B & CA & -16.30 & -22.10 & 5.80 \\\\\n",
      "Mixtral 8x7B & NCSE & 6.60 & 7.00 & -0.40 \\\\\n",
      "Mixtral 8x7B & SMH & -14.60 & -19.10 & 4.50 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diff_df = df_merged.loc[:, ['Model', 'Dataset' ,'Value_Full', 'Value_Instruct', 'Difference']].sort_values('Model')\n",
    "\n",
    "print(render_latex_with_formatting(diff_df, caption = 'The difference between the two prompts across all models and datasets', label= 'tab:prompt_diff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Value_Full</th>\n",
       "      <th>Value_Instruct</th>\n",
       "      <th>Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Claude 3 Haiku</td>\n",
       "      <td>NCSE</td>\n",
       "      <td>14.0</td>\n",
       "      <td>27.8</td>\n",
       "      <td>-13.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Claude 3 Haiku</td>\n",
       "      <td>CA</td>\n",
       "      <td>26.0</td>\n",
       "      <td>34.7</td>\n",
       "      <td>-8.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Claude 3 Haiku</td>\n",
       "      <td>SMH</td>\n",
       "      <td>38.4</td>\n",
       "      <td>35.7</td>\n",
       "      <td>2.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Claude 3 Opus</td>\n",
       "      <td>CA</td>\n",
       "      <td>48.2</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Claude 3 Opus</td>\n",
       "      <td>NCSE</td>\n",
       "      <td>64.1</td>\n",
       "      <td>62.7</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Claude 3 Opus</td>\n",
       "      <td>SMH</td>\n",
       "      <td>51.0</td>\n",
       "      <td>45.5</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>CA</td>\n",
       "      <td>44.2</td>\n",
       "      <td>44.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>NCSE</td>\n",
       "      <td>37.7</td>\n",
       "      <td>39.4</td>\n",
       "      <td>-1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GPT-3.5</td>\n",
       "      <td>SMH</td>\n",
       "      <td>39.2</td>\n",
       "      <td>42.9</td>\n",
       "      <td>-3.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>NCSE</td>\n",
       "      <td>60.4</td>\n",
       "      <td>59.8</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>CA</td>\n",
       "      <td>38.2</td>\n",
       "      <td>37.6</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>SMH</td>\n",
       "      <td>42.1</td>\n",
       "      <td>41.8</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Gemma 7B</td>\n",
       "      <td>CA</td>\n",
       "      <td>-38.0</td>\n",
       "      <td>-41.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemma 7B</td>\n",
       "      <td>NCSE</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Gemma 7B</td>\n",
       "      <td>SMH</td>\n",
       "      <td>-35.7</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>-22.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>NCSE</td>\n",
       "      <td>19.1</td>\n",
       "      <td>16.1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>CA</td>\n",
       "      <td>-9.4</td>\n",
       "      <td>14.2</td>\n",
       "      <td>-23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Llama 3</td>\n",
       "      <td>SMH</td>\n",
       "      <td>12.9</td>\n",
       "      <td>17.9</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mixtral 8x7B</td>\n",
       "      <td>CA</td>\n",
       "      <td>-16.3</td>\n",
       "      <td>-22.1</td>\n",
       "      <td>5.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mixtral 8x7B</td>\n",
       "      <td>NCSE</td>\n",
       "      <td>6.6</td>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mixtral 8x7B</td>\n",
       "      <td>SMH</td>\n",
       "      <td>-14.6</td>\n",
       "      <td>-19.1</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model Dataset  Value_Full  Value_Instruct  Difference\n",
       "2   Claude 3 Haiku    NCSE        14.0            27.8       -13.8\n",
       "16  Claude 3 Haiku      CA        26.0            34.7        -8.7\n",
       "9   Claude 3 Haiku     SMH        38.4            35.7         2.7\n",
       "20   Claude 3 Opus      CA        48.2            47.0         1.2\n",
       "6    Claude 3 Opus    NCSE        64.1            62.7         1.4\n",
       "13   Claude 3 Opus     SMH        51.0            45.5         5.5\n",
       "18         GPT-3.5      CA        44.2            44.1         0.1\n",
       "4          GPT-3.5    NCSE        37.7            39.4        -1.7\n",
       "11         GPT-3.5     SMH        39.2            42.9        -3.7\n",
       "5            GPT-4    NCSE        60.4            59.8         0.6\n",
       "19           GPT-4      CA        38.2            37.6         0.6\n",
       "12           GPT-4     SMH        42.1            41.8         0.3\n",
       "14        Gemma 7B      CA       -38.0           -41.0         3.0\n",
       "0         Gemma 7B    NCSE        -2.3             0.1        -2.4\n",
       "7         Gemma 7B     SMH       -35.7           -12.9       -22.8\n",
       "3          Llama 3    NCSE        19.1            16.1         3.0\n",
       "17         Llama 3      CA        -9.4            14.2       -23.6\n",
       "10         Llama 3     SMH        12.9            17.9        -5.0\n",
       "15    Mixtral 8x7B      CA       -16.3           -22.1         5.8\n",
       "1     Mixtral 8x7B    NCSE         6.6             7.0        -0.4\n",
       "8     Mixtral 8x7B     SMH       -14.6           -19.1         4.5"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_df.sort_values('Value_Full')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj4AAAHcCAYAAADWVqC4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAB8V0lEQVR4nO3dd1gU1/s28HtpS1mKKAgiSkdUbFiigGAFO8YSFQvW2KOxx6jYe+9JFGyxl/i1d6NiV7CBAoLYsAuCAQTm/cOX+bksIOAiZe/Pde11sWfOnHlmdmAfzpyZIxEEQQARERGRClAr7ACIiIiIvhcmPkRERKQymPgQERGRymDiQ0RERCqDiQ8RERGpDCY+REREpDKY+BAREZHKYOJDREREKoOJDxEREakMJj70Vf7+/pBIJHJlqampGDt2LCwtLaGmpgYfHx8AQEJCAvr16wczMzNIJBKMGDHi+wdcRERHR0MikWDBggWFHQr9fxmfSWBgoFiW1fld1BSHGImKCyY+KiYwMBASiUR8aWtro1y5cvDy8sKyZcvw4cOHXLWzfv16zJ8/Hx07dsSGDRswcuRIAMCsWbMQGBiIQYMGYdOmTejRo0dB7o7KuXfvHvz9/REdHV3YoRQYKysr+Pv7i+/PnDkjd85++erSpct3iaG48PPzg6enp1yZRCLB0KFDC2ybs2bNwr59+wqs/W+JIePvnar6+++/sWTJksIOo8jRKOwAqHBMmzYN1tbW+PTpE2JjY3HmzBmMGDECixYtwv79+1GtWjWx7u+//47x48fLrX/q1ClYWFhg8eLFCuU//PADpkyZ8l32Q9Xcu3cPU6dOhaenJ6ysrAo7nO9q+PDhqFOnjlyZqh2DomjWrFno2LGj2OurqjEURX///Tfu3Lmj0j3vWWHio6JatGiB2rVri+8nTJiAU6dOoXXr1mjbti1CQ0Oho6MDANDQ0ICGhvyp8vLlSxgZGSm0+/LlS1SuXFlpcaanpyMlJQXa2tpKa5OKJ3d3d3Ts2LGww6BvkJiYCD09vcIOQ6lSU1ORnp4OLS2twg6FcomXukjUuHFjTJo0CY8ePcLmzZvF8i/HF2SMkTh9+jTu3r0rXnLIuBwRFRWFgwcPiuUZl2SSk5MxZcoU2NnZQSqVwtLSEmPHjkVycrJcDBnd8lu2bEGVKlUglUpx5MgRAMDTp0/Rp08flC1bFlKpFFWqVMH69evl1s+IY8eOHZg5cybKly8PbW1tNGnSBBEREQr7fPnyZbRs2RKlSpWCnp4eqlWrhqVLl8rVCQsLQ8eOHWFsbAxtbW3Url0b+/fvz9OxXbx4MSpWrAgdHR14eHjgzp07CnW+tp3AwEB06tQJANCoUSO5Y//rr7+idOnSEARBrD9s2DBIJBIsW7ZMLHvx4gUkEglWr14tluX2swGAzZs3w8XFBTo6OjA2NkaXLl3w+PFjuTqenp6oWrUq7t27h0aNGkFXVxcWFhaYN29eno5ZXlhZWcHPz0+h3NPTU+HSjzItWLAADRo0QOnSpaGjowMXFxfs2rVLoV7Geb1v3z5UrVpVPH8zzu0vnT9/HnXq1IG2tjZsbW2xdu1apcacl9+R8PBwdOjQAWZmZtDW1kb58uXRpUsXxMXFifuVmJiIDRs2iOdjxueQ8Xfj3r176NatG0qVKgU3NzcA2X8ufn5+Cr146enpWLp0KZydnaGtrQ0TExN4e3vj2rVrX40ht/z8/CCTyfDw4UN4eXlBT08P5cqVw7Rp0+R+p74ct7dkyRLY2tpCKpXi3r17AD73eLu7u0NPTw9GRkZo164dQkND5baVcVwePHiA7t27w9DQECYmJpg0aRIEQcDjx4/Rrl07GBgYwMzMDAsXLszy89u+fTt+++03mJmZQU9PD23btpX7XfT09MTBgwfx6NEj8biwh/Qz9viQnB49euC3337DsWPH0L9/f4XlJiYm2LRpE2bOnImEhATMnj0bAODk5IRNmzZh5MiRKF++PEaNGiXWT09PR9u2bXH+/HkMGDAATk5OuH37NhYvXowHDx4oXJs/deoUduzYgaFDh6JMmTKwsrLCixcv8MMPP4hfICYmJjh8+DD69u2L+Ph4ha7cOXPmQE1NDaNHj0ZcXBzmzZsHX19fXL58Waxz/PhxtG7dGubm5vjll19gZmaG0NBQHDhwAL/88gsA4O7du3B1dYWFhQXGjx8PPT097NixAz4+Pti9ezfat2//1WO6ceNGfPjwAUOGDEFSUhKWLl2Kxo0b4/bt2yhbtmyut9OwYUMMHz4cy5Ytw2+//QYnJyfx2L979w6LFy/G3bt3UbVqVQDAuXPnoKamhnPnzmH48OFiGQA0bNgQAPL02cycOROTJk1C586d0a9fP7x69QrLly9Hw4YNcfPmTbkewHfv3sHb2xs//vgjOnfujF27dmHcuHFwdnZGixYtvnrMsvLhwwe8fv1arszY2BhqaoX3/9vSpUvRtm1b+Pr6IiUlBdu2bUOnTp1w4MABtGrVSq7u+fPnsWfPHgwePBj6+vpYtmwZOnTogJiYGJQuXRoAcPv2bTRv3hwmJibw9/dHamoqpkyZIp4nyvS135GUlBR4eXkhOTkZw4YNg5mZGZ4+fYoDBw7g/fv3MDQ0xKZNm9CvXz/UrVsXAwYMAADY2trKbadTp06wt7fHrFmz5JKI3Orbty8CAwPRokUL9OvXD6mpqTh37hwuXbqE2rVr5yqG3EhLS4O3tzd++OEHzJs3D0eOHMGUKVOQmpqKadOmydUNCAhAUlISBgwYAKlUCmNjY5w4cQItWrSAjY0N/P398d9//2H58uVwdXXFjRs3FJKOn376CU5OTpgzZw4OHjyIGTNmwNjYGGvXrkXjxo0xd+5cbNmyBaNHj0adOnXE39kMM2fOhEQiwbhx4/Dy5UssWbIETZs2RXBwMHR0dDBx4kTExcXhyZMn4pAEmUyW5+NSIgmkUgICAgQAwtWrV7OtY2hoKNSsWVN8P2XKFCHzqeLh4SFUqVJFYd2KFSsKrVq1kivbtGmToKamJpw7d06ufM2aNQIA4cKFC2IZAEFNTU24e/euXN2+ffsK5ubmwuvXr+XKu3TpIhgaGgofP34UBEEQTp8+LQAQnJychOTkZLHe0qVLBQDC7du3BUEQhNTUVMHa2lqoWLGi8O7dO7k209PTxZ+bNGkiODs7C0lJSXLLGzRoINjb2yvs/5eioqIEAIKOjo7w5MkTsfzy5csCAGHkyJF53s7OnTsFAMLp06fltvXy5UsBgLBq1SpBEATh/fv3gpqamtCpUyehbNmyYr3hw4cLxsbG4j7m9rOJjo4W1NXVhZkzZ8rVu337tqChoSFX7uHhIQAQNm7cKJYlJycLZmZmQocOHXI8ZlnJ+EyzekVFRQmC8Pm869Wrl8K6Hh4egoeHh/g+4zMJCAgQy7I6v3Mr47zLkJKSIlStWlVo3LixXDkAQUtLS4iIiBDLQkJCBADC8uXLxTIfHx9BW1tbePTokVh27949QV1dPd8xAhCGDBkivs/t78jNmzcFAMLOnTtzbF9PTy/LY59xXLt27aqwLPPnkqFXr15CxYoVxfenTp0SAAjDhw9XqPvl72l2MeRWr169BADCsGHD5Npv1aqVoKWlJbx69UoQhP87fwwMDISXL1/KtVGjRg3B1NRUePPmjVgWEhIiqKmpCT179hTLMo7LgAEDxLLU1FShfPnygkQiEebMmSOWv3v3TtDR0ZHbt4zPz8LCQoiPjxfLd+zYIQAQli5dKpa1atVK7njSZ7zURQpkMlmu7+7KjZ07d8LJyQmVKlXC69evxVfjxo0BAKdPn5ar7+HhITdOSBAE7N69G23atIEgCHJteHl5IS4uDjdu3JBro3fv3nLX3N3d3QEADx8+BADcvHkTUVFRGDFihMJYpYzLem/fvsWpU6fQuXNnsbfh9evXePPmDby8vBAeHo6nT59+df99fHxgYWEhvq9bty7q1auHQ4cOKW07JiYmqFSpEv79918AwIULF6Curo4xY8bgxYsXCA8PB/C5x8fNzU3cx9x+Nnv27EF6ejo6d+4sV8/MzAz29vYKn6FMJkP37t3F91paWqhbt654/PNj8uTJOH78uNzLzMws3+0pQ8Y4OOBzL1dcXBzc3d0VzkcAaNq0qVxPRLVq1WBgYCAek7S0NBw9ehQ+Pj6oUKGCWM/JyQleXl5Kj/1rvyOGhoYAgKNHj+Ljx4/53s7AgQPzve7u3bshkUiyvFmiIO7W+vLut4ze5ZSUFJw4cUKuXocOHWBiYiK+f/78OYKDg+Hn5wdjY2OxvFq1amjWrJn4u/6lfv36iT+rq6ujdu3aEAQBffv2FcuNjIzg6OiY5e9Nz549oa+vL77v2LEjzM3Ns9wWyeOlLlKQkJAAU1NTpbUXHh6O0NBQuT8UX3r58qXce2tra7n3r169wvv37/HHH3/gjz/+yFUbX35xAECpUqUAfP5yAoDIyEgAEC8LZSUiIgKCIGDSpEmYNGlSttv9MqnJir29vUKZg4MDduzYodTtuLu7i3/0zp07h9q1a6N27dowNjbGuXPnULZsWYSEhKBbt27iOrn9bMLDwyEIQpb7AgCamppy78uXL6/wxVSqVCncunUrx33IibOzM5o2bZrv9QvCgQMHMGPGDAQHB8uNicrqSznzOQl8PiYZ5+SrV6/w33//ZXmMHR0dlf6F9rXfEWtra/z6669YtGgRtmzZAnd3d7Rt21Ycl5JbmX+f8yIyMhLlypWTSyYKipqaGmxsbOTKHBwcAEDh8RGZ9+nRo0cAPn9OmTk5OeHo0aMKA7szH39DQ0Noa2ujTJkyCuVv3rxRaDfzeSKRSGBnZ1eiH3WhLEx8SM6TJ08QFxcHOzs7pbWZnp4OZ2dnLFq0KMvllpaWcu+//C86Y30A6N69O3r16pVlG1/efg98/g8qK0IexhhkbHf06NHZ/setjOOkrO24ubnhzz//xMOHD3Hu3Dm4u7tDIpHAzc0N586dQ7ly5ZCeni7+Z5+x7dx8Nunp6ZBIJDh8+HCWxzbz2AFlHP+8yO6//7S0tGxj+Vbnzp1D27Zt0bBhQ6xatQrm5ubQ1NREQEAA/v77b4X63/uYfE1u4lm4cCH8/Pzwzz//4NixYxg+fDhmz56NS5cuoXz58rnaTubfZ+Dz55XVfqelpeUy+sKV1T7lVVbHv6idIyUVEx+Ss2nTJgBQate6ra0tQkJC0KRJk3x1T5uYmEBfXx9paWlK+48/45LDnTt3sm0z478/TU3Nb9puxmWmLz148EAc7JiX7eR0/DISmuPHj+Pq1avis5caNmyI1atXo1y5ctDT04OLi4u4Tm4/G1tbWwiCAGtra/G/4KKkVKlSeP/+vUL5o0ePFP6LV5bdu3dDW1sbR48ehVQqFcsDAgLy1Z6JiQl0dHSyPF/u37+f7zi/lbOzM5ydnfH7778jKCgIrq6uWLNmDWbMmAEgf5ecSpUqleXlm4yekwy2trY4evQo3r59m2OvjzIue6Wnp+Phw4dy5/eDBw8AfP15URUrVgSQ9ecUFhaGMmXKKP02/szniSAIiIiIkPsnUJUf3pgTjvEh0alTpzB9+nRYW1vD19dXae127twZT58+xZ9//qmw7L///kNiYmKO66urq6NDhw7YvXt3lreBv3r1Ks8x1apVC9bW1liyZInCF2bGf1empqbw9PTE2rVr8fz583xvd9++fXJjdK5cuYLLly+LdzflZTsZfzyz+pK3trYWHyr56dMnuLq6AvicEEVGRmLXrl344Ycf5J7JlNvP5scff4S6ujqmTp2q8N+nIAhZdsV/T7a2trh06RJSUlLEsgMHDijcaq9M6urqkEgkcr0U0dHR+X6Ksbq6Ory8vLBv3z7ExMSI5aGhoTh69Oi3hptn8fHxSE1NlStzdnaGmpqa3GU9PT29LM/HnNja2iIsLEzu3A4JCcGFCxfk6nXo0AGCIGDq1KkKbXx5HuYnhqysWLFCrv0VK1ZAU1MTTZo0yXE9c3Nz1KhRAxs2bJCL486dOzh27Bhatmz5zbFllnG3aIZdu3bh+fPncndN6unpiY8eoP/DHh8VdfjwYYSFhSE1NRUvXrzAqVOncPz4cVSsWBH79+9X6gMDe/TogR07dmDgwIE4ffo0XF1dkZaWhrCwMOzYsQNHjx6Ve5hiVubMmYPTp0+jXr166N+/PypXroy3b9/ixo0bOHHiBN6+fZunmNTU1LB69Wq0adMGNWrUQO/evWFubo6wsDDcvXtX/KJZuXIl3Nzc4OzsjP79+8PGxgYvXrzAxYsX8eTJE4SEhHx1W3Z2dnBzc8OgQYOQnJyMJUuWoHTp0hg7dqxYJ7fbqVGjBtTV1TF37lzExcVBKpWicePG4pgsd3d3bNu2Dc7OzuKYjVq1akFPTw8PHjyQG98D5P6zsbW1xYwZMzBhwgRER0fDx8cH+vr6iIqKwt69ezFgwACMHj06T5+BMvXr1w+7du2Ct7c3OnfujMjISGzevDlftzXnVqtWrbBo0SJ4e3ujW7duePnyJVauXAk7O7t8j2WaOnUqjhw5And3dwwePBipqalYvnw5qlSp8k3jo/Lj1KlTGDp0KDp16gQHBwekpqZi06ZN4j8iGVxcXHDixAksWrQI5cqVg7W1NerVq5dj23369MGiRYvg5eWFvn374uXLl1izZg2qVKmC+Ph4sV6jRo3Qo0cPLFu2DOHh4fD29kZ6ejrOnTuHRo0aiYOR8xNDZtra2jhy5Ah69eqFevXq4fDhwzh48CB+++23bMfAfWn+/Plo0aIF6tevj759+4q3sxsaGhbI9CfGxsZwc3ND79698eLFCyxZsgR2dnZyjyFxcXHB9u3b8euvv6JOnTqQyWRo06aN0mMpdr7vTWRU2DJuZ894aWlpCWZmZkKzZs2EpUuXyt0emeFbb2cXhM+3+c6dO1eoUqWKIJVKhVKlSgkuLi7C1KlThbi4OLEeMt16+6UXL14IQ4YMESwtLQVNTU3BzMxMaNKkifDHH3+IdTJu9cx8C25WtzELgiCcP39eaNasmaCvry/o6ekJ1apVk7u9WBAEITIyUujZs6dgZmYmaGpqChYWFkLr1q2FXbt2ZRln5m3Onz9fWLhwoWBpaSlIpVLB3d1dCAkJUaif2+38+eefgo2NjXiL85e3tq9cuVIAIAwaNEhunaZNmwoAhJMnTypsN7efjSAIwu7duwU3NzdBT09P0NPTEypVqiQMGTJEuH//vlgnu3Mj863KuZXdZ5rZwoULBQsLC0EqlQqurq7CtWvXCvx29nXr1gn29vaCVCoVKlWqJAQEBGTZXnbndVa34Z89e1ZwcXERtLS0BBsbG2HNmjXfFGPmbef2d+Thw4dCnz59BFtbW0FbW1swNjYWGjVqJJw4cUJuvbCwMKFhw4aCjo6OAEDcn4yYM24Fz2zz5s2CjY2NoKWlJdSoUUM4evRoludIamqqMH/+fKFSpUqClpaWYGJiIrRo0UK4fv36V2PIrV69egl6enpCZGSk0Lx5c0FXV1coW7asMGXKFCEtLU3hGM2fPz/Ldk6cOCG4uroKOjo6goGBgdCmTRvh3r17cnWyOy4ZMWSW+fcp4/PbunWrMGHCBMHU1FTQ0dERWrVqJfcYBEEQhISEBKFbt26CkZGRAIC3tv9/EkHgqCkiIlJdfn5+2LVrFxISEgo7lK86c+YMGjVqhJ07d3IKl3ziGB8iIiJSGUx8iIiISGUw8SEiIiKVwTE+REREpDLY40NEREQqg4kPERERqQwmPvRdBQYGQiKRcCI9JQoPD0fz5s1haGgIiUSS7ycHU+Gen35+fl+dGoGIvh0THyrWPn78CH9/f5w5c6awQyk0vXr1wu3btzFz5kxs2rQp26dgR0dHQyKRZPuaM2fOd448dzKSkYyXhoYGLCws4OfnJzcVSHHw7Nkz+Pv7Izg4uLBDEWU+L9TV1VGhQgW0b9++SMWZX/wbQZlxygr6rnr06IEuXbrITer4LT5+/CjO4+Pp6amUNouT//77DxcvXsTEiRPFx/d/TdeuXbOcO6hmzZrKDk+ppk2bBmtrayQlJeHSpUsIDAzE+fPncefOHaVOsVKQnj17hqlTp8LKygo1atSQW/bnn38iPT29cALD/50XaWlpCA0NxerVq3H48GFcunRJIdbiRNX/RpAiJj70Xamrq0NdXb2ww/iqxMREpc+mXBAyJnk0MjLK9Tq1atVC9+7d87QdQRCQlJQEHR0dhWVJSUnQ0tKCmlr+O5Bzc7xbtGgh9mb169cPZcqUwdy5c7F//3507tw539suKjQ1NQt1+5nPC1dXV7Rt2xarV6/G2rVrv6nt4vL7RKqBl7rou8pqDIWVlRVat26N8+fPo27dutDW1oaNjQ02btyYY1vR0dHi5IFTp04Vu+q/nBAwLCwMHTt2hLGxMbS1tVG7dm3s378/y5jOnj2LwYMHw9TUFOXLlwfw+T/EqlWr4tatW/Dw8ICuri7s7Oywa9cuAMDZs2dRr1496OjowNHRESdOnJBr+8OHDxgxYgSsrKwglUphamqKZs2a4caNG189Vjdv3kSLFi1gYGAAmUyGJk2a4NKlS+Jyf39/VKxYEQAwZswYSCQSpY0RyfhMMiYp1dHRwdq1a3HmzBlIJBJs27YNv//+OywsLKCrqytOLLlz5064uLhAR0cHZcqUQffu3RUuR/n5+UEmkyEyMhItW7aEvr4+fH198xyju7s7ACAyMlKuPDefOQDcvXsXjRs3ho6ODsqXL48ZM2Zk2eOS+Zz68hj5+fnJlb1//x4jR44UP+/y5cujZ8+eeP36Nc6cOYM6deoAAHr37i2er4GBgeJxyfz5JSYmYtSoUbC0tIRUKoWjoyMWLFiAzE8hkUgkGDp0KPbt24eqVatCKpWiSpUqOHLkSE6HMEeNGzcGAERFRYllly9fhre3NwwNDaGrqwsPDw+FGdX9/f0hkUhw7949dOvWDaVKlYKbm5u4fPPmzahbty50dXVRqlQpNGzYEMeOHZNr4/Dhw3B3d4eenh709fXRqlUr3L17V65Oxnn09OlT+Pj4QCaTwcTEBKNHj0ZaWhqAr/+NuHXrFvz8/GBjYwNtbW2YmZmhT58+ePPmjcLxOHPmDGrXrg1tbW3Y2tpi7dq14r5mtnnzZvH3wNjYGF26dMHjx49ze+ipgLHHh4qEiIgIdOzYEX379kWvXr2wfv16+Pn5wcXFBVWqVMlyHRMTE6xevRqDBg1C+/bt8eOPPwIAqlWrBuDzF5urqyssLCwwfvx46OnpYceOHfDx8cHu3bvRvn17ufYGDx4MExMTTJ48GYmJiWL5u3fv0Lp1a3Tp0gWdOnXC6tWr0aVLF2zZsgUjRozAwIED0a1bN8yfPx8dO3bE48ePoa+vDwAYOHAgdu3ahaFDh6Jy5cp48+YNzp8/j9DQUNSqVSvb43H37l24u7vDwMAAY8eOhaamJtauXQtPT08x2frxxx9hZGSEkSNHipcpZDLZV4/1x48f8fr1a4VyIyMjaGj835+E+/fvo2vXrvj555/Rv39/ODo6isumT58OLS0tjB49GsnJydDS0kJgYCB69+6NOnXqYPbs2Xjx4gWWLl2KCxcu4ObNm3K9UqmpqfDy8oKbmxsWLFgAXV3dr8adWUbynDELfcZxy81nHhsbi0aNGiE1NVWs98cff2TZo5VbCQkJcHd3R2hoKPr06YNatWrh9evX2L9/P548eQInJydMmzYNkydPxoABA8TErUGDBlm2JwgC2rZti9OnT6Nv376oUaMGjh49ijFjxuDp06dYvHixXP3z589jz549GDx4MPT19bFs2TJ06NABMTExKF26dJ73JyOhzFj31KlTaNGiBVxcXDBlyhSoqakhICAAjRs3xrlz51C3bl259Tt16gR7e3vMmjVLTNSmTp0Kf39/NGjQANOmTYOWlhYuX76MU6dOoXnz5gCATZs2oVevXvDy8sLcuXPx8eNHrF69Gm5ubrh586ZccpiWlgYvLy/Uq1cPCxYswIkTJ7Bw4ULY2tpi0KBBX/0bcfz4cTx8+BC9e/eGmZkZ7t69iz/++AN3797FpUuXxKTm5s2b8Pb2hrm5OaZOnYq0tDRMmzYty1nbZ86ciUmTJqFz587o168fXr16heXLl6Nhw4YKvwdUSApvflRSRRmzw0dFRYllFStWFAAI//77r1j28uVLQSqVCqNGjcqxvVevXgkAhClTpigsa9KkieDs7CwkJSWJZenp6UKDBg0Ee3t7hZjc3NyE1NRUuTY8PDwEAMLff/8tloWFhQkABDU1NeHSpUti+dGjRxVm/jY0NMx2tvmc+Pj4CFpaWkJkZKRY9uzZM0FfX19o2LChWPa12aK/lFE3u9fFixfFuhmfyZEjR+TayJgZ2sbGRvj48aNYnpKSIpiamgpVq1YV/vvvP7H8wIEDAgBh8uTJYlmvXr0EAML48eNzdSwyPp8TJ04Ir169Eh4/fizs2rVLMDExEaRSqfD48WOxbm4/8xEjRggAhMuXL4tlL1++FAwNDRXOz+zOr8wzq0+ePFkAIOzZs0ehbnp6uiAIgnD16lWFcyRD5pnJ9+3bJwAQZsyYIVevY8eOgkQiESIiIuRi1NLSkisLCQkRAAjLly9X2NaXMs6LqVOnCq9evRJiY2OFM2fOCDVr1hQACLt37xbS09MFe3t7wcvLS9wXQRCEjx8/CtbW1kKzZs3EsozZx7t27Sq3nfDwcEFNTU1o37693IznXx6fDx8+CEZGRkL//v3llsfGxgqGhoZy5Rnn0bRp0+Tq1qxZU3BxcRHf5/Q34stzOMPWrVsV/h61adNG0NXVFZ4+fSq3PxoaGsKXX6PR0dGCurq6MHPmTLk2b9++LWhoaCiUU+HgpS4qEipXriz+Bwx87s1xdHTEw4cP89Xe27dvcerUKXTu3BkfPnzA69ev8fr1a7x58wZeXl4IDw9XuATTv3//LMcfyWQydOnSRXzv6OgIIyMjODk5oV69emJ5xs9fxmxkZITLly/j2bNnuY49LS0Nx44dg4+PD2xsbMRyc3NzdOvWDefPnxcvLeXHgAEDcPz4cYVX5cqV5epZW1vDy8sryzZ69eol1zty7do1vHz5EoMHD5YbaNyqVStUqlQJBw8eVGhj0KBBeYq7adOmMDExgaWlJTp27Ag9PT3s379fvCyZl8/80KFD+OGHH+R6KUxMTPJ1yS3D7t27Ub16dYWeRABZXg75mkOHDkFdXR3Dhw+XKx81ahQEQcDhw4flyps2bQpbW1vxfbVq1WBgYJDr36EpU6bAxMQEZmZm8PT0RGRkJObOnYsff/wRwcHBCA8PR7du3fDmzRvx2CYmJqJJkyb4999/FS4TDhw4UO79vn37kJ6ejsmTJyuMB8s4PsePH8f79+/RtWtXcRuvX7+Guro66tWrh9OnTyvEnXk77u7uud7nL8/hpKQkvH79Gj/88AMAiJej09LScOLECfj4+KBcuXJifTs7O7Ro0UKuvT179iA9PR2dO3eWi9/MzAz29vZZxk/fHy91UZFQoUIFhbJSpUrh3bt3+WovIiICgiBg0qRJmDRpUpZ1Xr58CQsLC/G9tbV1lvXKly+v8MVlaGgIS0tLhTIAcjHPmzcPvXr1gqWlJVxcXNCyZUv07NlTLqHJ7NWrV/j48aPcpaUMTk5OSE9Px+PHj7O9BPg19vb2aNq06VfrZXc8slr26NEjAMgy5kqVKuH8+fNyZRoaGmLCklsrV66Eg4MD4uLisH79evz7779ydwfm5TN/9OiRXNKaIav4cysyMhIdOnTI9/qZPXr0COXKlRMvm2ZwcnISl3/pW3+HBgwYgE6dOkFNTQ1GRkaoUqWKeHzDw8MBfE54sxMXFyd32THzORIZGQk1NTWFBPtLGdvJGF+UmYGBgdx7bW1thctNednnt2/fYurUqdi2bRtevnwptywuLg7A53Pmv//+g52dncL6mcvCw8MhCALs7e2z3F5hD2Cnz5j4UJGQ3Z1eQj6nksv473P06NHZ9lpk/qOV3fiO7GLLTcydO3eGu7s79u7di2PHjmH+/PmYO3cu9uzZo/DfYlGT03iXbxkLAwBSqTTPd4HVrVtXvKvLx8cHbm5u6NatG+7fvw+ZTJavz/xbZAygLSq+9Xcop4Q449jOnz8/21vbM48vy885krGdTZs2wczMTGH5l2PQgOz3Obc6d+6MoKAgjBkzBjVq1BDPI29v73w9WiA9PR0SiQSHDx/OtveYCh8THyrWsruEkNGjoqmpmavejYJkbm6OwYMHY/DgwXj58iVq1aqFmTNnZpv4mJiYQFdXF/fv31dYFhYWBjU1NYXepsKWcXfZ/fv3Ff5bv3//vrhcWdTV1TF79mw0atQIK1aswPjx4/P0mVesWFHsXcgca2alSpXC+/fv5cpSUlLw/PlzuTJbW1vcuXMnx+3m5ZJXxYoVceLECXz48EGu1ycsLExc/r1kXEIzMDDI9++Tra0t0tPTce/evWyTp4ztmJqaKu33Nrtj/u7dO5w8eRJTp07F5MmTxfLM54WpqSm0tbURERGh0EbmMltbWwiCAGtrazg4OCgheioIHONDxVrG3UCZv5hMTU3h6emJtWvXKnxBAf/3/JuClJaWJnaXfxlXuXLlkJycnO166urqaN68Of755x+52/5fvHiBv//+G25ubgpd/oWtdu3aMDU1xZo1a+T27fDhwwgNDUWrVq2Uvk1PT0/UrVsXS5YsQVJSUp4+85YtW+LSpUu4cuWK3PItW7YorGdra4t///1XruyPP/5Q6PHp0KEDQkJCsHfvXoU2MnpdMp5lk/l8zUrGwwRXrFghV7548WJIJJLv2mPo4uICW1tbLFiwAAkJCQrLc/P75OPjAzU1NUybNk2hNyXj+Hh5ecHAwACzZs3Cp0+f8rWdzLL7G5HRI5O5R2zJkiUK9Zo2bYp9+/bJjdWLiIhQGGf1448/Ql1dHVOnTlVoVxCELG+Tp++PPT5UrOno6KBy5crYvn07HBwcYGxsjKpVq6Jq1apYuXIl3Nzc4OzsjP79+8PGxgYvXrzAxYsX8eTJE4SEhBRobB8+fED58uXRsWNHVK9eHTKZDCdOnMDVq1excOHCHNedMWMGjh8/Djc3NwwePBgaGhpYu3YtkpOTMW/evG+K68aNG9i8ebNCua2tLerXr5+vNjU1NTF37lz07t0bHh4e6Nq1q3g7u5WVFUaOHPlNMWdnzJgx6NSpEwIDAzFw4MBcf+Zjx47Fpk2b4O3tjV9++UW8nb1ixYq4deuW3Db69euHgQMHokOHDmjWrBlCQkJw9OhRlClTRiGWXbt2oVOnTujTpw9cXFzw9u1b7N+/H2vWrEH16tVha2sLIyMjrFmzBvr6+tDT00O9evWyHE/Vpk0bNGrUCBMnTkR0dDSqV6+OY8eO4Z9//sGIESPkBjIXNDU1Nfz1119o0aIFqlSpgt69e8PCwgJPnz7F6dOnYWBggP/97385tmFnZ4eJEydi+vTpcHd3x48//gipVIqrV6+iXLlymD17NgwMDLB69Wr06NEDtWrVQpcuXWBiYoKYmBgcPHgQrq6uCong1+T0N6Jhw4aYN28ePn36BAsLCxw7dkzuuUUZ/P39cezYMbi6umLQoEFiQlq1alW5aT1sbW0xY8YMTJgwAdHR0fDx8YG+vj6ioqKwd+9eDBgwAKNHj85T/FQACuVeMlJZ2d3O3qpVK4W6Hh4egoeHx1fbDAoKElxcXAQtLS2F21YjIyOFnj17CmZmZoKmpqZgYWEhtG7dWti1a5dCTFevXs0yhipVqiiUZxczAPH29eTkZGHMmDFC9erVBX19fUFPT0+oXr26sGrVqq/ukyAIwo0bNwQvLy9BJpMJurq6QqNGjYSgoCC5Osq8nf3LW7Oz27+M29l37tyZ5Ta2b98u1KxZU5BKpYKxsbHg6+srPHnyRK5Or169BD09vVwcgc9y+nzS0tIEW1tbwdbWVnwUQW4+c0EQhFu3bgkeHh6Ctra2YGFhIUyfPl1Yt26dwvmZlpYmjBs3TihTpoygq6sreHl5CREREQq3swuCILx580YYOnSoYGFhIWhpaQnly5cXevXqJbx+/Vqs888//wiVK1cWb4XOuLU98+3sgvD59u6RI0cK5cqVEzQ1NQV7e3th/vz5creUC4L8efelrGLMLC/n0M2bN4Uff/xRKF26tCCVSoWKFSsKnTt3Fk6ePCnWybid/dWrV1m2sX79evEcKVWqlODh4SEcP35crs7p06cFLy8vwdDQUNDW1hZsbW0FPz8/4dq1a2Kd7M6jjO1/Kbu/EU+ePBHat28vGBkZCYaGhkKnTp2EZ8+eZXn7+8mTJ4WaNWsKWlpagq2trfDXX38Jo0aNErS1tRVi2L17t+Dm5ibo6ekJenp6QqVKlYQhQ4YI9+/fz/H40vchEYR8jh4lIiJSYT4+Prh7926W48Wo6OIYHyIioq/477//5N6Hh4fj0KFDnPi0GGKPDxER0VeYm5uL83o9evQIq1evRnJyMm7evJntc3uoaOLgZiIioq/w9vbG1q1bERsbC6lUivr162PWrFlMeooh9vgQERGRyuAYHyIiIlIZTHyIiIhIZXCMTybp6el49uwZ9PX18zWjMhEREX1/giDgw4cPKFeuXI5zATLxyeTZs2dFbh4kIiIiyp3Hjx+jfPny2S5n4pNJxmSAjx8/LnLzIREREVHW4uPjYWlpKTepb1aY+GSScXnLwMCAiQ8REVEx87VhKhzcTERERCqDiQ8RERGpDCY+REREpDKY+BAREZHKYOJDREREKoOJDxEREakMJj5ERESkMpj4EBERkcpg4kNEREQqg4kPERERqQwmPkRERKQymPgQERGRymDiQ0RERCqDiQ8RERGpDI3CDoCIiOhbxUxzVlpbFSbfVlpbVPSwx4eIiIhUBhMfIiIiUhm81EVEVEyEzjyltLacJjZWWltExQl7fIiIiEhllKjEx9/fHxKJRO5VqVKlwg6LiIiIiogSd6mrSpUqOHHihPheQ6PE7SIRERHlU4nLCjQ0NGBmZlbYYRAREVERVKIudQFAeHg4ypUrBxsbG/j6+iImJibH+snJyYiPj5d7ERERUclUohKfevXqITAwEEeOHMHq1asRFRUFd3d3fPjwIdt1Zs+eDUNDQ/FlaWn5HSMmIiKi76lEJT4tWrRAp06dUK1aNXh5eeHQoUN4//49duzYke06EyZMQFxcnPh6/Pjxd4yYiIiIvqcSN8bnS0ZGRnBwcEBERES2daRSKaRS6XeMioiIiApLierxySwhIQGRkZEwNzcv7FCIiIioCChRic/o0aNx9uxZREdHIygoCO3bt4e6ujq6du1a2KERERFREVCiLnU9efIEXbt2xZs3b2BiYgI3NzdcunQJJiYmhR0aERERFQElKvHZtm1bYYdAVOLFTHNWWlsVJt9WWltERLlRoi51EREREeWEiQ8RERGpDCY+REREpDKY+BAREZHKYOJDREREKoOJDxEREakMJj5ERESkMpj4EBERkcpg4kNEREQqg4kPERERqQwmPkRERKQymPgQERGRymDiQ0RERCqDiQ8RERGpDCY+REREpDKY+BAREZHK0CjsAKh4OtvQQ2ltefx7VmltEVHx4TJmo9La2quvtKaohGOPDxEREakMJj5ERESkMnipi4gKjetyV6W1dWHYBaW1RUQlF3t8iIiISGUw8SEiIiKVwcSHiIiIVAYTHyIiIlIZTHyIiIhIZTDxISIiIpXBxIeIiIhUBhMfIiIiUhlMfIiIiEhlMPEhIiIilcHEh4iIiFQGEx8iIiJSGUx8iIiISGUw8SEiIiKVoVHYARAVRaEzTymtLaeJjZXWFhERfRv2+BAREZHKYI8PERER5Utx7B1njw8RERGpDCY+REREpDKY+BAREZHKYOJDREREKoOJDxEREakMJj5ERESkMng7O5UoM7t3VEo7PzoNVko7RERFjbL+TgLF828le3yIiIhIZTDxISIiIpVRIhOflStXwsrKCtra2qhXrx6uXLlS2CERERFREVDixvhs374dv/76K9asWYN69ephyZIl8PLywv3792FqalrY4RGRilH18RRERU2J6/FZtGgR+vfvj969e6Ny5cpYs2YNdHV1sX79+sIOjYiIiApZierxSUlJwfXr1zFhwgSxTE1NDU2bNsXFixezXCc5ORnJycni+/j4+AKPk4iIKC9WjPpfYYdQYpSoxOf169dIS0tD2bJl5crLli2LsLCwLNeZPXs2pk6dqtQ4XMZsVFpbe/XnK62trqUMlNbWhX8vKK0tZf5CT9y8S2ltlSTKPCevz7+ttLaUdxYBZxt6KK2t23VGK60tnpPZuz6/pxJbU15brstdldLOrJ3K+4od+u9ZpbUFtFFiW8VPibvUlVcTJkxAXFyc+Hr8+HFhh0REREQFpET1+JQpUwbq6up48eKFXPmLFy9gZmaW5TpSqRRSqfR7hEdERESFrET1+GhpacHFxQUnT54Uy9LT03Hy5EnUr1+/ECMjIiKioqBE9fgAwK+//opevXqhdu3aqFu3LpYsWYLExET07t27sEMjIiKiQlbiEp+ffvoJr169wuTJkxEbG4saNWrgyJEjCgOei4sKk5U3kBRKGrBHRERUXJW4xAcAhg4diqFDhxZ2GERERFTElKgxPkREREQ5YeJDREREKoOJDxEREakMJj5ERESkMpj4EBERkcpg4kNEREQqg4kPERERqQwmPkRERKQymPgQERGRymDiQ0RERCqDiQ8RERGpDCY+REREpDKY+BAREZHKKJGzs1PxMnRhm8IOgYiIVAR7fIiIiEhlMPEhIiIilcHEh4iIiFQGEx8iIiJSGUx8iIiISGUw8SEiIiKVwcSHiIiIVAaf41MArs/vWdghEBERURbY40NEREQqg4kPERERqQwmPkRERKQymPgQERGRymDiQ0RERCqDiQ8RERGpDCY+REREpDKY+BAREZHKYOJDREREKoOJDxEREakMJj5ERESkMpj4EBERkcpg4kNEREQqg7OzExFlMnRhm8IOgYgKCHt8iIiISGUw8SEiIiKVwcSHiIiIVAYTHyIiIlIZTHyIiIhIZTDxISIiIpXBxIeIiIhUBhMfIiIiUhlMfIiIiEhlMPEhIiIilVGiEh8rKytIJBK515w5cwo7LCIiIioiStxcXdOmTUP//v3F9/r6+oUYDRERERUlJS7x0dfXh5mZWWGHQUREREVQibrUBQBz5sxB6dKlUbNmTcyfPx+pqak51k9OTkZ8fLzci4iIiEqmEtXjM3z4cNSqVQvGxsYICgrChAkT8Pz5cyxatCjbdWbPno2pU6d+xyiJqCB4/Hu2sEMgomKgyPf4jB8/XmHAcuZXWFgYAODXX3+Fp6cnqlWrhoEDB2LhwoVYvnw5kpOTs21/woQJiIuLE1+PHz/+XrtGRERE31mR7/EZNWoU/Pz8cqxjY2OTZXm9evWQmpqK6OhoODo6ZllHKpVCKpV+a5hERERUDBT5xMfExAQmJib5Wjc4OBhqamowNTVVclRERERUHBX5xCe3Ll68iMuXL6NRo0bQ19fHxYsXMXLkSHTv3h2lSpUq7PCIiIioCCgxiY9UKsW2bdvg7++P5ORkWFtbY+TIkfj1118LOzQiIiIqIkpM4lOrVi1cunSpsMMgIiKiIqzI39VFREREpCxMfIiIiEhlMPEhIiIilcHEh4iIiFQGEx8iIiJSGd+U+ERERODo0aP477//AACCICglKCIiIqKCkK/E582bN2jatCkcHBzQsmVLPH/+HADQt29fjBo1SqkBEhERESlLvhKfkSNHQkNDAzExMdDV1RXLf/rpJxw5ckRpwREREREpU74eYHjs2DEcPXoU5cuXlyu3t7fHo0ePlBIYERERkbLlq8cnMTFRrqcnw9u3bznTORERERVZ+Up83N3dsXHjRvG9RCJBeno65s2bh0aNGiktOCIiIiJlytelrnnz5qFJkya4du0aUlJSMHbsWNy9exdv377FhQsXlB0jERERkVLkq8enatWqePDgAdzc3NCuXTskJibixx9/xM2bN2Fra6vsGImIiIiUIt+zsxsaGmLixInKjIWIiIioQOWrxycgIAA7d+5UKN+5cyc2bNjwzUERERERFYR8JT6zZ89GmTJlFMpNTU0xa9asbw6KiIiIqCDkK/GJiYmBtbW1QnnFihURExPzzUERERERFYR8JT6mpqa4deuWQnlISAhKly79zUERERERFYR8JT5du3bF8OHDcfr0aaSlpSEtLQ2nTp3CL7/8gi5duig7RiIiIiKlyNddXdOnT0d0dDSaNGkCDY3PTaSnp6Nnz54c40NERERFVr4SHy0tLWzfvh3Tp09HSEgIdHR04OzsjIoVKyo7PiIiIiKlyfdzfADAwcEBDg4OyoqFiIiIqEDlK/FJS0tDYGAgTp48iZcvXyI9PV1u+alTp5QSHBEREZEy5Svx+eWXXxAYGIhWrVqhatWqkEgkyo6LiIiISOnylfhs27YNO3bsQMuWLZUdDxEREVGBydft7FpaWrCzs1N2LEREREQFKl+Jz6hRo7B06VIIgqDseIiIiIgKTL4udZ0/fx6nT5/G4cOHUaVKFWhqasot37Nnj1KCIyIiIlKmfCU+RkZGaN++vbJjISIiIipQ+Up8AgIClB0HERERUYHL1xgfAEhNTcWJEyewdu1afPjwAQDw7NkzJCQkKC04IiIiImXKV4/Po0eP4O3tjZiYGCQnJ6NZs2bQ19fH3LlzkZycjDVr1ig7TiIiIqJvlq8en19++QW1a9fGu3fvoKOjI5a3b98eJ0+eVFpwRERERMqUrx6fc+fOISgoCFpaWnLlVlZWePr0qVICIyIiIlK2fPX4pKenIy0tTaH8yZMn0NfX/+agiIiIiApCvhKf5s2bY8mSJeJ7iUSChIQETJkyhdNYEBERUZGVr0tdCxYsgLe3NypXroykpCR069YN4eHhKFOmDLZu3arsGImIiL6bC8MuKKWdszs9lNIOKVe+Eh9LS0uEhIRg+/btCAkJQUJCAvr27QtfX1+5wc5ERERERUmeE59Pnz6hUqVKOHDgAHx9feHr61sQcREREREpXZ7H+GhqaiIpKakgYiEiIiIqUPka3DxkyBDMnTsXqampyo6HiIiIqMDka4zP1atXcfLkSRw7dgzOzs7Q09OTW87Z2YmIiKgoyvfs7B06dFB2LEREREQFirOz50NaWho+ffpU2GHkmamOqdLa4jivgqOpqQl1dfXCDoOIqETKV+IDfJ6d/cyZM4iMjES3bt2gr6+PZ8+ewcDAADKZTJkxFhmCICA2Nhbv378v7FDyZbjzcKW1FRUVpbS2SJGRkRHMzMwgkUgKOxQiohKFs7PnQUbSY2pqCl1d3WL3pSS8FpTWlnUZa6W1Rf9HEAR8/PgRL1++BACYm5sXckRERCVLvhKfjNnZQ0JCULp0abG8ffv26N+/v9KC+9LMmTNx8OBBBAcHQ0tLK8tel5iYGAwaNAinT5+GTCZDr169MHv2bGho5LtjS5SWliYmPV/uc3Gippmvm/iypK2trbS2SF7GQ0BfvnwJU1NTXvYiIlKiYjM7e0pKCjp16oT69etj3bp1CsvT0tLQqlUrmJmZISgoCM+fP0fPnj2hqamJWbNmffP2M8b06OrqfnNbRF+TcZ59+vSJiQ8RkRIVm9nZp06dipEjR8LZ2TnL5ceOHcO9e/ewefNm1KhRAy1atMD06dOxcuVKpKSkKC2O4nZ5i4onnmdERAWjxMzOfvHiRTg7O6Ns2bJimZeXF+Lj43H37t1s10tOTkZ8fLzci4iIiEqmfCU+CxcuxIULF+RmZ8+4zDV37lxlx5grsbGxckkPAPF9bGxstuvNnj0bhoaG4svS0rJA46TPzpw5A4lEkqc75KysrOQSbiIiorzKV+JTvnx5hISEYOLEiRg5ciRq1qyJOXPm4ObNmzA1zf2zYsaPHw+JRJLjKywsLD8h5tqECRMQFxcnvh4/flyg2ysuJgyfACczJ/iP9VdYNm38NEgkEvj5+X33uIiIiL5Frgc316pVCydPnkSpUqUwbdo0jB49+ptnZx81atRXvzxtbGxy1ZaZmRmuXLkiV/bixQtxWXakUimkUmmutqFqzC3McWjfIYyfOh7aOp/v4kpOSsbBvQdRoUKFQo6OiIgo73Ld4xMaGorExEQAnwcaJyQkfPPGTUxMUKlSpRxfme8cy079+vVx+/Zt8fknAHD8+HEYGBigcuXK3xyrKqrsXBlm5cxw/NBxsez4oeMwtzBHzZo1xbLk5GQMHz4cpqam0NbWhpubG65evSrX1qFDh+Dg4AAdHR00atQI0dHRCts7f/483N3doaOjA0tLSwwfPlw854iIiJQh1z0+NWrUQO/eveHm5gZBELBgwYJsn9A8efJkpQWYISYmBm/fvkVMTAzS0tIQHBwMALCzs4NMJkPz5s1RuXJl9OjRA/PmzUNsbCx+//13DBkyhD063+DHrj9iz7Y9aNOhDQBg99bdaN+lPUKvh4p1xo4di927d2PDhg2oWLEi5s2bBy8vL0RERMDY2BiPHz/Gjz/+iCFDhmDAgAG4du0aRo0aJbedyMhIeHt7Y8aMGVi/fj1evXqFoUOHYujQoSo/RQoRESlPrhOfwMBATJkyBQcOHIBEIsHhw4ezfDCgRCIpkMRn8uTJ2LBhg/g+o8fh9OnT8PT0hLq6Og4cOIBBgwahfv360NPTQ69evTBt2jSlx6JK2nZoi8WzFuPp48/PZ7p59SYWrVkkJj6JiYlYvXo1AgMD0aJFCwDAn3/+iePHj2PdunUYM2YMVq9eDVtbWyxcuBAA4OjoiNu3b8sNhJ89ezZ8fX0xYsQIAIC9vT2WLVsGDw8PrF69mg9MJCIipch14uPo6Iht27YBANTU1HDy5Mk8DWT+VoGBgQgMDMyxTsWKFXHo0KHvE5CKMC5jDI+mHti3fR8EQYBHEw+UKl1KXB4ZGYlPnz7B1dVVLNPU1ETdunURGvo5OQoNDUW9evXk2q1fv77c+5CQENy6dQtbtmwRywRBQHp6OqKiouDk5FQQu0dERComX4Obp0yZUmInIiVFP3b5ETN+mwEAmDR7UoFsIyEhAT///DOGD1ecSJUDqYmISFnyNbh52rRpShncTMWDe2N3fPr0CampqXBr5Ca3zNbWFlpaWrhw4YJY9unTJ1y9elUcVO7k5KRwx92lS5fk3teqVQv37t2DnZ2dwiu3A9yJiIi+ptgMbqbCo66ujoPnDoo/f0lPTw+DBg3CmDFjYGxsjAoVKmDevHn4+PEj+vbtCwAYOHAgFi5ciDFjxqBfv364fv26wmXLcePG4YcffsDQoUPRr18/6Onp4d69ezh+/DhWrFjxXfaTiIhKvmIzuJkKl0w/+0ubc+bMQXp6Onr06IEPHz6gdu3aOHr0KEqV+jwWqEKFCti9ezdGjhyJ5cuXo27dupg1axb69OkjtlGtWjWcPXsWEydOhLu7OwRBgK2tLX766acC3zciIlIdEkEQhLyupKamhtjY2O86uPl7iY+Ph6GhIeLi4mBgYCCWJyUlISoqCtbW1sX2DqOwF8p7CnalspWU1hYpUvb55jJmoxKi+uz6/J5Ka4uoJDvb0ENpbXn8e1ZpbZVU2X1/Z5brHp8vpaen5zswIiIiosKS68Rn//79aNGiBTQ1NbF///4c67Zt2/abAyMiIiJStlwnPj4+PuLlLR8fn2zrSSQSpKWlKSM2IiIiIqXKdeLz5eUtXuoiIiKi4ijPY3zS09MRGBiIPXv2IDo6GhKJBDY2NujQoQN69OgBiURSEHESERERfbNcP8AQ+DyFQNu2bdGvXz88ffoUzs7OqFKlCqKjo+Hn54f27dsXVJxERERE3yxPPT6BgYH4999/cfLkSTRq1Ehu2alTp+Dj44ONGzeiZ0/e7kpERERFT556fLZu3YrffvtNIekBgMaNG2P8+PFyk0wSERERFSV5Snxu3boFb2/vbJe3aNECISEh3xwUERERUUHIU+Lz9u1blC1bNtvlZcuWxbt37745KCIiIqKCkKcxPmlpaVnOz5VBXV0dqamp3xxUcaPM6QByIz9TBsTGxmLWpFk4e+IsYp/HQl9fHxWsK6BNhzbw6ewDHV2dAoj0+/Pz88OGDRsUyitXroy7d+9mWcfY2Bh16tTBvHnzUK1ate8WKxERfX95SnwEQYCfnx+kUmmWy5OTk5USFCnXw4cP4erqCl19XYyYMAIOTg7QkmrhQegD7Ni0A2XNy6KxV+PCDlMpli5dijlz5ojvU1NTUb16dXTq1Emunre3NwICAgB8Tgp///13tG7dGjExMd81XiIi+r7ydKmrV69eMDU1haGhYZYvU1NT3tFVBA0ePBgaGhrYeWQnWrRrAVsHW1hWtEQT7yZYu2UtGjX/v8Hq8XHx+P3X39GgcgPUtqsNvw5+CLv7f5Obrpi/Au2btMf69etRoUIFyGQyDB48GGlpaZg3bx7MzMxgamqKmTNnysUgkUiwdu1atG7dGrq6unBycsLFixcREREBT09P6OnpoUGDBoiMjBTXiYyMRLt27VC2bFnIZDLUqVMHJ06cyHFfDQ0NYWZmJr6uXbuGd+/eoXfv3nL1pFKpWKdGjRoYP348Hj9+jFevXn3LoSYioiIuTz0+Gf8hU/Hx5s0bHDt2DLNmzYKunm6Wdb586OSI/iOgra2NP/7+AzIDGXZs3IHenXrj8IXDMCplBACIiY7B4cOHceTIEURGRqJjx454+PAhHBwccPbsWQQFBaFPnz5o2rQp6tWrJ7Y9ffp0LFq0CIsWLcK4cePQrVs32NjYYMKECahQoQL69OmDoUOH4vDhwwCAhIQEtGzZEjNnzoRUKsXGjRvRpk0b3L9/HxUqVMjV/q9btw5NmzZFxYoVs62TkJCAzZs3w87ODqVLl85Vu0REVDzla3Z2Kj4iIiIgCAIcHR3lyutXro+UpBQAQNfeXTF60mhcv3wdt2/exoU7F6Al1QIAjPUfi5NHTuLYgWPo3KMzAEBIF7B+/Xro6+ujcuXKaNSoEe7fv49Dhw5BTU0Njo6OmDt3Lk6fPi2X+PTu3RudO39uY9y4cahfvz4mTZoELy8vAMAvv/wi1zNTvXp1VK9eXXw/ffp07N27F/v378fQoUO/uu/Pnj3D4cOH8ffffyssO3DgAGQyGQAgMTER5ubmOHDgANTU8tQJSkRExQwTHxW14/AOpKenY+zgsUhJ+ZwAhd0Nw8fEj6jvVF+ublJSEmKi/2/sSznLctDX1xffly1bFurq6nJJQ9myZfHy5Uu5dr4cOJxxd6Czs7NcWVJSEuLj42FgYICEhAT4+/vj4MGDeP78OVJTU/Hff//lehzOhg0bYGRklOWkuo0aNcLq1asBAO/evcOqVavQokULXLlyJcfeISIiKt6Y+JRwdnZ2kEgkuH//PpwaOInllhUtAQBS7f8bqP4x8SNMyppgwx7Fu6IMDAzEnzU1NeWWSSSSLMsyT2b7ZZ2My2tZlWWsN3r0aBw/fhwLFiyAnZ0ddHR00LFjRzFRy4kgfO6V6tGjB7S0tBSW6+npwc7OTnz/119/wdDQEH/++SdmzJjx1faJiKh4YuJTwpUuXRrNmjXDihUr0Lxz82zH+QBA5WqV8frla2ioa8CigsV3jDJrFy5ckJsDLiEhAdHR0bla9+zZs4iIiEDfvn1zVV8ikUBNTQ3//fdffsMlIqJigAMaVMCqVauQmpqKTt6dcGjfIUQ+iERURBT279qPqIgoqKupAwAaNGyAGrVrYGjvobhw5gKexjzFzas3sWT2EtwJvvPd47a3t8eePXsQHByMkJAQdOvWTaEXKTvr1q1DvXr1ULVq1SyXJycnIzY2FrGxsQgNDcWwYcOQkJCANm3aKHMXiIioiGGPjwqwtbXFzZs3Meb3MVg8azFePH8BTS1N2DnYofeg3ujq1xXA/7/lfMtaLJm9BL+N+A3v3rxDGdMyqP1DbZQ2+f53Oy1atAh9+vRBgwYNUKZMGYwbNw7x8fFfXS8uLg67d+/G0qVLs61z5MgRmJubAwD09fVRqVIl7Ny5E56ensoKn4iIiiCJIAhCYQdRlMTHx8PQ0BBxcXFy41qSkpIQFRUFa2traGtrF2KE+Rf2IuzrlXKpUtlKSmuLFCn7fFPm08Xz8+RwIlV0tqGH0try+Pes0toqqbL7/s6Ml7qIiIhIZTDxISIiIpXBxIeIiIhUBhMfIiIiUhlMfIiIiEhlMPEhIiIilcHEh4iIiFQGEx8iIiJSGUx8iIiISGUw8SEFPdv3xKxJswpt+56enhgxYkShbZ+IiEouztWlBDHTnL/r9ipMvp2n+n5+ftiwYQN+6vkT/Of5yy2bNn4atgZuhU9nH8xeNhsAsGz9Mmhqauap/ffv32Pfvn15iutbbNmyBfPmzUN4eDgMDQ3RokULzJ8/H6VL525OMX9/f0ydOlWhXFdXF4mJiVnWMTAwQLVq1TBjxgx4eCjvUfRERPT9MPFREZaWljjyzxEErAmAjo4OgM/zQR3edxgVKlSAoY7h/82/VbZgYvj06VOeEqrsXLhwAT179sTixYvRpk0bPH36FAMHDkT//v2xZ8+eXLUxevRoDBw4UK6sSZMmqFOnjlxZlSpVcOLECQDA27dvsWDBArRu3RpPnjyBoaHhN+8LERF9X7zUpSJq1aoFS0tLucRgz549qFChAmrWrClX98tLTWFhYdDV1cXff/8tLt+xYwd0dHRw7949+Pv7Y8OGDfjnn38gkUggkUhw5swZREdHQyKRYPv27fDw8IC2tja2bNmCN2/eoGvXrrCwsICuri6cnZ2xdevWPO3LxYsXYWVlheHDh8Pa2hpubm74+eefceXKFQCfE7oqVapgwIAB4jqRkZHQ19fH+vXrAQAymQxmZmbi68WLF7h37x769u0rty0NDQ2xTuXKlTFt2jQkJCTgwYMHeYqZiIiKBiY+KqRPnz4ICAgQ369fvx69e/fOcZ1KlSphwYIFGDx4MGJiYvDkyRMMHDgQc+fOReXKlTF69Gh07twZ3t7eeP78OZ4/f44GDRqI648fPx6//PILQkND4eXlhaSkJLi4uODgwYO4c+cOBgwYgB49eohJS27Ur18fjx8/xqFDhyAIAl68eIFdu3ahZcuWACAmWRkJWVpaGrp3745mzZqhT58+Wbb5119/wcHBAe7u7tluNzk5GQEBATAyMoKjo2Ou4yUioqKDl7pUSPfu3TFhwgQ8evQIwOdLRtu2bcOZM2dyXG/w4ME4dOgQunfvDi0tLdSpUwfDhg0D8LnnREdHB8nJyTAzM1NYd8SIEfjxxx/lykaPHi3+PGzYMBw9ehQ7duxA3bp1c7Ufrq6u2LJlC3766SckJSUhNTUVbdq0wcqVK8U6NWrUwIwZM9CvXz906dIFjx49woEDB7JsLykpCVu2bMH48eMVlt2+fRsymQwA8PHjR+jr62P79u0wMDDIVaxERFS0MPFRISYmJmjVqhUCAwMhCAJatWqFMmXK5Grd9evXw8HBAWpqarh79y4kEkmu1qtdu7bc+7S0NMyaNQs7duzA06dPkZKSguTkZOjq6uZ6P+7du4dffvkFkydPhpeXF54/f44xY8Zg4MCBWLdunVhv1KhR2LdvH1asWIHDhw9nO/B57969+PDhA3r16qWwzNHREfv37wcAfPjwAdu3b0enTp1w+vRphX0jIqKij4mPiunTpw+GDh0KAHI9JF8TEhKCxMREqKmp4fnz5zA3N8/Venp6enLv58+fj6VLl2LJkiVwdnaGnp4eRowYgZSUlFzHMnv2bLi6umLMmDEAgGrVqkFPTw/u7u6YMWOGGNvLly/x4MEDqKurIzw8HN7e3lm299dff6F169YoW1ZxVLeWlhbs7OzE9zVr1sS+ffuwZMkSbN68OdcxExFR0cDER8V4e3sjJSUFEokEXl5euVrn7du38PPzw8SJE/H8+XP4+vrixo0b4t1hWlpaSEtLy1VbFy5cQLt27dC9e3cAQHp6Oh48eIDKlSvneh8+fvwIDQ35U1ddXR0AIAiCWNanTx84Ozujb9++6N+/P5o2bQonJye59aKionD69GmxVyc31NXV8d9//+W6PhERFR0c3Kxi1NXVERoainv37onJwtcMHDgQlpaW+P3337Fo0SKkpaXJjdOxsrLCrVu3cP/+fbx+/RqfPn3Kti17e3scP34cQUFBCA0Nxc8//4wXL17kaR/atGmDPXv2YPXq1Xj48CEuXLiA4cOHo27duihXrhyAz71ZFy9exIYNG+Dr6wsfHx/4+voq9CytX78e5ubmaNGiRZbbSk1NRWxsLGJjYxEeHo4ZM2bg3r17aNeuXZ5iJiKiooGJjwoyMDDI9eDcjRs34tChQ9i0aRM0NDSgp6eHzZs3488//8Thw4cBAP3794ejoyNq164NExMTXLhwIdv2fv/9d9SqVQteXl7w9PSEmZkZfHx88hS/n58fFi1ahBUrVqBq1aro1KkTHB0dxVv1w8LCMGbMGKxatQqWlpYAgFWrVuH169eYNGmS2E56ejoCAwPh5+eXbRJ49+5dmJubw9zcHDVq1MCOHTuwevVq9OzZM08xExFR0SARvrw2UITNnDkTBw8eRHBwMLS0tPD+/XuFOlkNuN26dSu6dOmS6+3Ex8fD0NAQcXFxcslBUlISoqKiYG1tDW1t7XztA1FuKft8cxmzUQlRfXZ9PpM+otw421B5T3j3+Pes0toqqbL7/s6s2IzxSUlJQadOnVC/fn25O3cyCwgIkBvEamRk9B2iIyIiouKg2CQ+GXMmBQYG5ljPyMgoy+fJEBEREZW4MT5DhgxBmTJlULduXaxfvx7F5EoeERERfQfFpscnN6ZNm4bGjRtDV1cXx44dw+DBg5GQkIDhw4dnu05ycjKSk5PF9/Hx8d8jVCIiIioEhdrjM378eHFiy+xeYWFhuW5v0qRJcHV1Rc2aNTFu3DiMHTsW8+fPz3Gd2bNnw9DQUHxl3AVEREREJU+h9viMGjUKfn5+OdaxsbHJd/v16tXD9OnTkZycDKlUmmWdCRMm4NdffxXfx8fHM/khIiIqoQo18TExMYGJiUmBtR8cHIxSpUplm/QAgFQqzXE5ERERlRzFZoxPTEwM3r59i5iYGKSlpSE4OBgAYGdnB5lMhv/973948eIFfvjhB2hra+P48eOYNWuW3BOGiYiISLUVm7u6Jk+ejJo1a2LKlClISEhAzZo1UbNmTVy7dg0AoKmpiZUrV6J+/fqoUaMG1q5di0WLFmHKlCmFHHnxIZFIsG/fvgLfjqenJ0aMGFHg2ylIVlZWWLJkifj+ex07IiL6NsUm8QkMDIQgCAovT09PAJ8n37x58yY+fPiAhIQEBAcH4+eff4aaWrHZxQIVGxuLYcOGwcbGBlKpFJaWlmjTpg1OnjxZ2KHly88//wxbW1vo6OjAxMQE7dq1++pA+OwSrsDAwDw/6PLq1asYMGBAntYhIqLCV2wudRVlrstdv+v2LgzLfi6srERHR8PV1RVGRkaYP38+nJ2d8enTJxw9ehRDhgzJ051zRYWLiwt8fX1RoUIFvH37Fv7+/mjevDmioqJyPfnqtyjIsWlERFRw2B2iAgYPHgyJRIIrV66gQ4cOcHBwQJUqVfDrr7/i0qVL2a43btw4ODg4QFdXFzY2Npg0aZLczOt+fn4KE4yOGDFC7IUDgMTERPTs2RMymQzm5uZYuHChwnaSk5MxevRoWFhYQE9PD/Xq1cOZM2dy3KcBAwagYcOGsLKyQq1atTBjxgw8fvwY0dHRuTkkOYqMjES7du1QtmxZyGQy1KlTBydOnJCrk/lSV2ZTpkyBubk5bt26lWWP0r59+7KcW46IiAoWE58S7u3btzhy5AiGDBkCPT09heU5XeLR19dHYGAg7t27h6VLl+LPP//E4sWL87T9MWPG4OzZs/jnn39w7NgxnDlzBjdu3JCrM3ToUFy8eBHbtm3DrVu30KlTJ3h7eyM8PDxX20hMTERAQACsra2V8iiChIQEtGzZEidPnsTNmzfh7e2NNm3aICYm5qvrCoKAYcOGYePGjTh37hyqVav2zfEQEZHy8FJXCRcREQFBEFCpUqU8r/v777+LP1tZWWH06NHYtm0bxo4dm6v1ExISsG7dOmzevBlNmjQBAGzYsAHly5cX68TExCAgIAAxMTEoV64cAGD06NE4cuQIAgICMGvWrGzbX7VqFcaOHYvExEQ4Ojri+PHj0NLSyjGmVatW4a+//pIrS01NlZsBvXr16qhevbr4fvr06di7dy/279+PoUOHZtt2amoqunfvjps3b+L8+fOwsLDIMRYiIvr+mPiUcN8yV9n27duxbNkyREZGIiEhAampqTAwMMj1+pGRkUhJSUG9evXEMmNjYzg6Oorvb9++jbS0NDg4OMitm5ycjNKlS+fYvq+vL5o1a4bnz59jwYIF6Ny5My5cuCCXxGS1zsSJE+XK9uzZI5dgJSQkwN/fHwcPHsTz58+RmpqK//7776s9PiNHjoRUKsWlS5dQpkyZHOsSEVHhYOJTwtnb2+d56g8AuHjxInx9fTF16lR4eXnB0NAQ27Ztkxujo6amppBYfTkGKDcSEhKgrq6O69evKwxKlslkOa6bMc2Ivb09fvjhB5QqVQp79+5F165dc1zHzs5OrszU1FTu/ejRo3H8+HEsWLAAdnZ20NHRQceOHZGSkpJjPM2aNcPWrVtx9OhR+Pr6iuXKOE5ERKQcTHxKOGNjY3h5eWHlypUYPny4wjif9+/fZznOJygoCBUrVpTrHXn06JFcHRMTE9y5c0euLDg4GJqamgAAW1tbaGpq4vLly6hQoQIA4N27d3jw4AE8PDwAADVr1kRaWhpevnwJd3f3fO9nxuMNvpxwNr8uXLgAPz8/tG/fHsDn5Cw3g6bbtm2LNm3aoFu3blBXV0eXLl0AfD5OHz58QGJionj8Mx7ASURE3xcHN6uAlStXIi0tDXXr1sXu3bsRHh6O0NBQLFu2DPXr189yHXt7e8TExGDbtm2IjIzEsmXLsHfvXrk6jRs3xrVr17Bx40aEh4djypQpcomQTCZD3759MWbMGJw6dQp37tyBn5+f3LOVHBwc4Ovri549e2LPnj2IiorClStXMHv2bBw8eDDL2B4+fIjZs2fj+vXriImJQVBQEDp16gQdHR20bNnym4+Xvb099uzZg+DgYISEhKBbt25IT0/P1brt27fHpk2b0Lt3b+zatQvA5znjdHV18dtvvyEyMhJ///03AgMDvzlOIiLKOyY+KsDGxgY3btxAo0aNMGrUKFStWhXNmjXDyZMnsXr16izXadu2LUaOHImhQ4eiRo0aCAoKwqRJk+TqeHl5YdKkSRg7dizq1KmDDx8+oGfPnnJ15s+fD3d3d7Rp0wZNmzaFm5sbXFxc5OoEBASgZ8+eGDVqFBwdHeHj44OrV6+KvUSZaWtr49y5c2jZsiXs7Ozw008/QV9fH0FBQQqXrfJj0aJFKFWqFBo0aIA2bdrAy8sLtWrVyvX6HTt2xIYNG9CjRw/s2bMHxsbG2Lx5Mw4dOgRnZ2ds3boV/v7+3xwnERHlnUT4ltGvJVB8fDwMDQ0RFxcnN5A3KSkJUVFRsLa2znHwLJEyKPt8cxmzUQlRfXZ9fs+vVyIinG3oobS2PP49q7S2Sqrsvr8zY48PERERqQwmPkRERKQymPgQERGRymDiQ0RERCqDiQ8RERGpDCY+REREpDKY+BAREZHKYOJDREREKoOJDxEREakMJj4EiUSCffv2FXYYREREBY6Jjwrw8/ODj49PYYeRLz///DNsbW2ho6MDExMTtGvXDmFhYYUdFhERFVMahR1ASaDM+VhyQ5XmbHFxcYGvry8qVKiAt2/fwt/fH82bN0dUVBTU1dULOzwiIipm2ONDCsaNGwcHBwfo6urCxsYGkyZNwqdPn8Tl/v7+qFGjBtavX48KFSpAJpNh8ODBSEtLw7x582BmZgZTU1PMnDlTrt1FixbB2dkZenp6sLS0xODBg5GQkJBjLAMGDEDDhg1hZWWFWrVqYcaMGXj8+DGio6MLYteJiKiEY48PKdDX10dgYCDKlSuH27dvo3///tDX18fYsWPFOpGRkTh8+DCOHDmCyMhIdOzYEQ8fPoSDgwPOnj2LoKAg9OnTB02bNkW9evUAAGpqali2bBmsra3x8OFDDB48GGPHjsWqVatyFVdiYiICAgJgbW0NS0vLAtl3IiIq2Zj4kILff/9d/NnKygqjR4/Gtm3b5BKf9PR0rF+/Hvr6+qhcuTIaNWqE+/fv49ChQ1BTU4OjoyPmzp2L06dPi4nPiBEj5NqdMWMGBg4c+NXEZ9WqVRg7diwSExPh6OiI48ePQ0tLS7k7TUREKoGJDynYvn07li1bhsjISCQkJCA1NRUGBgZydaysrKCvry++L1u2LNTV1aGmpiZX9vLlS/H9iRMnMHv2bISFhSE+Ph6pqalISkrCx48foaurm208vr6+aNasGZ4/f44FCxagc+fOuHDhArS1tZW410REpAo4xofkXLx4Eb6+vmjZsiUOHDiAmzdvYuLEiUhJSZGrp6mpKfdeIpFkWZaeng4AiI6ORuvWrVGtWjXs3r0b169fx8qVKwFAoe3MDA0NYW9vj4YNG2LXrl0ICwvD3r17v3VXiYhIBbHHh+QEBQWhYsWKmDhxolj26NGjb273+vXrSE9Px8KFC8VeoR07duS5HUEQIAgCkpOTvzkmIiJSPUx8VERcXByCg4PlykqXLq0wSNje3h4xMTHYtm0b6tSpg4MHDyqld8XOzg6fPn3C8uXL0aZNG1y4cAFr1qzJcZ2HDx9i+/btaN68OUxMTPDkyRPMmTMHOjo6aNmy5TfHREREqoeXulTEmTNnULNmTbnX1KlTFeq1bdsWI0eOxNChQ1GjRg0EBQVh0qRJ37z96tWrY9GiRZg7dy6qVq2KLVu2YPbs2Tmuo62tjXPnzqFly5aws7PDTz/9BH19fQQFBcHU1PSbYyIiItUjEQRBKOwgipL4+HgYGhoiLi5ObkBvUlISoqKiYG1tzUG1VOCUfb65jNmohKg+uz6/p9LaIirJlPlwW1V6cG1+Zff9nRl7fIiIiEhlMPEhIiIilcHEh4iIiFQGEx8iIiJSGUx8iIiISGUw8SEiIiKVwcSHiIiIVAYTHyIiIlIZTHyIiIhIZTDxISIiIpXBxEeFxMbG4pdffoGdnR20tbVRtmxZuLq6YvXq1fj48SMAwMrKChKJBBKJBHp6eqhVqxZ27typsCyrl5+fn8I2z58/D1dXV5QuXRo6OjqoVKkSFi9enGOc0dHRWbZ/6dIlpR8TIiJSLZydXQlWjPrfd93e0IVt8rzOw4cP4erqCiMjI8yaNQvOzs6QSqW4ffs2/vjjD1hYWKBt27YAgGnTpqF///6Ij4/HwoUL8dNPP8HCwgJXr15FWloaACAoKAgdOnTA/fv3xTlRdHR0FLarp6eHoUOHolq1atDT08P58+fx888/Q09PDwMGDMgx5hMnTqBKlSri+9KlS+d5v4mIiL7ExEdFDB48GBoaGrh27Rr09PTEchsbG7Rr1w5fzlWrr68PMzMzmJmZYeXKldi8eTP+97//yc2mbmxsDAAwNTWFkZFRttvNmAk+g5WVFfbs2YNz5859NfEpXbo0zMzM8rqrRERE2eKlLhXw5s0bHDt2DEOGDJFLer4kkUiyLNfQ0ICmpiZSUlKUEsvNmzcRFBQED4+vz1rctm1bmJqaws3NDfv371fK9omISLUx8VEBEREREAQBjo6OcuVlypSBTCaDTCbDuHHjFNZLSUnB7NmzERcXh8aNG39TDOXLl4dUKkXt2rUxZMgQ9OvXL9u6MpkMCxcuxM6dO3Hw4EG4ubnBx8eHyQ8REX2zYpH4REdHo2/fvrC2toaOjg5sbW0xZcoUhV6IW7duwd3dHdra2rC0tMS8efMKKeLi4cqVKwgODkaVKlWQnJwslo8bNw4ymQy6urqYO3cu5syZg1atWn21vYwkSiaTYeDAgXLLzp07h2vXrmHNmjVYsmQJtm7dmm07ZcqUwa+//op69eqhTp06mDNnDrp374758+fnf2eJiIhQTMb4hIWFIT09HWvXroWdnR3u3LmD/v37IzExEQsWLAAAxMfHo3nz5mjatCnWrFmD27dvo0+fPjAyMvrqWJKSzs7ODhKJBPfv35crt7GxAaA4KHnMmDHw8/ODTCZD2bJls70MlllwcLD4c8aA5wzW1tYAAGdnZ7x48QL+/v7o2rVrrvehXr16OH78eK7rExERZaVYJD7e3t7w9vYW39vY2OD+/ftYvXq1mPhs2bIFKSkpWL9+PbS0tFClShUEBwdj0aJFKp/4lC5dGs2aNcOKFSswbNiwbMf5ZChTpgzs7OzyvJ3crpOeni7Xw5QbwcHBMDc3z3NMREREXyoWiU9W4uLixDuLAODixYto2LAhtLS0xDIvLy/MnTsX7969Q6lSpQojzCJj1apVcHV1Re3ateHv749q1apBTU0NV69eRVhYGFxcXApkuytXrkSFChVQqVIlAMC///6LBQsWYPjw4WKdFStWYO/evTh58iQAYMOGDdDS0hLvBtuzZw/Wr1+Pv/76q0BiJCIi1VEsE5+IiAgsX75c7O0BPj+cL+NySoayZcuKy7JLfJKTk+V6H+Lj4wsg4sJna2uLmzdvYtasWZgwYQKePHkCqVSKypUrY/To0Rg8eHCBbDc9PR0TJkxAVFQUNDQ0YGtri7lz5+Lnn38W67x+/RqRkZFy602fPh2PHj2ChoYGKlWqhO3bt6Njx44FEiMREakOifDlA1y+s/Hjx2Pu3Lk51gkNDRV7CwDg6dOn8PDwgKenp1wPQPPmzWFtbY21a9eKZffu3UOVKlVw7949ODk5Zdm+v78/pk6dqlAeFxcnN04lKSkJUVFRsLa2hra2dq73kSg/lH2+uYzZqISoPrs+v6fS2iIqyc42/PpjO3LL49+zSmurpIqPj4ehoaHC93dmhdrjM2rUqCynOfhSxgBcAHj27BkaNWqEBg0a4I8//pCrZ2ZmhhcvXsiVZbzP6SF4EyZMwK+//iq+j4+Ph6WlZW53gYiIiIqRQk18TExMYGJikqu6T58+RaNGjeDi4oKAgACoqcnfiV+/fn1MnDgRnz59gqamJgDg+PHjcHR0zHF8j1QqhVQqzf9OEBERUbFRLJ7j8/TpU3h6eqJChQpYsGABXr16hdjYWMTGxop1unXrBi0tLfTt2xd3797F9u3bsXTpUrneHCIiIlJtxWJw8/HjxxEREYGIiAiUL19eblnGECVDQ0NxWgYXFxeUKVMGkydPVvlb2YmIiOj/FIvEx8/P76tjgQCgWrVqOHfuXMEHRERERMVSsbjURURERKQMTHyIiIhIZTDxISIiIpXBxIeIiIhUBhMfIiIiUhlMfFRIbGwsfvnlF9jZ2UFbWxtly5aFq6srVq9ejY8fPwIArKysIJFIIJFIoKenh1q1amHnzp0Ky7J6fe3OuwsXLkBDQwM1atQo4D0lIiLKWrG4nb2om9n9+06eOXHzrjyv8/DhQ7i6usLIyAizZs2Cs7MzpFIpbt++jT/++AMWFhZo27YtAGDatGno378/4uPjsXDhQvz000+wsLDA1atXkZaWBgAICgpChw4dcP/+fXFOFB0dnWy3//79e/Ts2RNNmjRRmFqEiIjoe2HioyIGDx4MDQ0NXLt2DXp6emK5jY0N2rVrhy/nqtXX14eZmRnMzMywcuVKbN68Gf/73/8we/ZssY6xsTEAwNTUFEZGRl/d/sCBA9GtWzeoq6tj3759StsvIiKivOClLhXw5s0b8anWXyY9X5JIJFmWa2hoQFNTEykpKfnefkBAAB4+fIgpU6bkuw0iIiJlYOKjAiIiIiAIAhwdHeXKy5QpA5lMBplMhnHjximsl5KSgtmzZyMuLg6NGzfO17bDw8Mxfvx4bN68GRoa7GAkIqLCxcRHhV25cgXBwcGoUqUKkpOTxfJx48ZBJpNBV1cXc+fOxZw5c9CqVauvtpeRRMlkMgwcOBBpaWno1q0bpk6dCgcHh4LcFSIiolzhv+AqwM7ODhKJBPfv35crt7GxAaA4KHnMmDHw8/ODTCZD2bJls70MlllwcLD4s4GBAT58+IBr167h5s2bGDp0KAAgPT0dgiBAQ0MDx44dy3dPEhERUX4w8VEBpUuXRrNmzbBixQoMGzYs23E+GcqUKQM7O7s8byfzOunp6bh9+7Zc2apVq3Dq1Cns2rUL1tbWed4GERHRt2DioyJWrVoFV1dX1K5dG/7+/qhWrRrU1NRw9epVhIWFwcXFRenbVFNTQ9WqVeXKTE1Noa2trVBORET0PTDxURG2tra4efMmZs2ahQkTJuDJkyeQSqWoXLkyRo8ejcGDBxd2iERERAVOInz5ABdCfHw8DA0NERcXJz6YDwCSkpIQFRUFa2traGtrF2KEpAqUfb65jNmohKg+uz6/p9LaIirJzjb0UFpbHv+eVVpbJVV239+Z8a4uIiIiUhlMfIiIiEhlMPEhIiIilcHEh4iIiFQGEx8iIiJSGUx8iIiISGUw8SEiIiKVwcSHiIiIVAYTHyIiIlIZTHxIJJFIsG/fvgLfjqenJ0aMGFHg2yEiIsqMc3WpiNjYWMycORMHDx7E06dPYWpqiho1amDEiBFo0qRJYYeXZz///DNOnDiBZ8+eQSaToUGDBpg7dy4qVaqU43p3797F1KlTcfr0acTHx6NixYro0qULxo8fD11d3e8U/ffHaSaIiD5j4qMEoTNPfdftOU1snKf60dHRcHV1hZGREebPnw9nZ2d8+vQJR48exZAhQxAWFlZAkRYcFxcX+Pr6okKFCnj79i38/f3RvHlzREVFQV1dPct1Ll26hKZNm6Jp06Y4ePAgypYtiytXrmDUqFE4efIkTp8+DS0tre+8J0RE9D3xUpcKGDx4MCQSCa5cuYIOHTrAwcEBVapUwa+//opLly5lu964cePg4OAAXV1d2NjYYNKkSfj06ZO43M/PDz4+PnLrjBgxAp6enuL7xMRE9OzZEzKZDObm5li4cKHCdpKTkzF69GhYWFhAT08P9erVw5kzZ3LcpwEDBqBhw4awsrJCrVq1MGPGDDx+/BjR0dFZ1hcEAX379oWTkxP27NmDunXromLFiujUqRP+97//4eLFi1i8eLFYXyKRYPXq1WjRogV0dHRgY2ODXbt2icvPnDkDiUSC9+/fi2XBwcGQSCRiDI8ePUKbNm1QqlQp6OnpoUqVKjh06FCO+0VERAWLiU8J9/btWxw5cgRDhgyBnp6ewnIjI6Ns19XX10dgYCDu3buHpUuX4s8//5RLDnJjzJgxOHv2LP755x8cO3YMZ86cwY0bN+TqDB06FBcvXsS2bdtw69YtdOrUCd7e3ggPD8/VNhITExEQEABra2tYWlpmWSc4OBj37t3Dr7/+CjU1+dO+evXqaNq0KbZu3SpXPmnSJHTo0AEhISHw9fVFly5dEBoamut9HzJkCJKTk/Hvv//i9u3bmDt3LmQyWa7XJyIi5WPiU8JFRERAEISvjn3Jyu+//44GDRrAysoKbdq0wejRo7Fjx45cr5+QkIB169ZhwYIFaNKkCZydnbFhwwakpqaKdWJiYhAQEICdO3fC3d0dtra2GD16NNzc3BAQEJBj+6tWrYJMJoNMJsPhw4dx/PjxbC9VPXjwAADg5OSU5XInJyexToZOnTqhX79+cHBwwPTp01G7dm0sX7481/sfExMDV1dXODs7w8bGBq1bt0bDhg1zvT4RESkfx/iUcIIg5Hvd7du3Y9myZYiMjERCQgJSU1NhYGCQ6/UjIyORkpKCevXqiWXGxsZwdHQU39++fRtpaWlwcHCQWzc5ORmlS5fOsX1fX180a9YMz58/x4IFC9C5c2dcuHAB2tra2a6Tl+NRv359hffBwcG5Xn/48OEYNGgQjh07hqZNm6JDhw6oVq1artcnIiLlY49PCWdvbw+JRJLnAcwXL16Er68vWrZsiQMHDuDmzZuYOHEiUlJSxDpqamoKicSXY4ByIyEhAerq6rh+/TqCg4PFV2hoKJYuXZrjuoaGhrC3t0fDhg2xa9cuhIWFYe/evVnWzUissrtUFRoaqpB85STjctmX+5953/v164eHDx+iR48euH37dp57jIiISPmY+JRwxsbG8PLywsqVK5GYmKiw/MvBuV8KCgpCxYoVMXHiRNSuXRv29vZ49OiRXB0TExM8f/5cruzLHhFbW1toamri8uXLYtm7d+/kLinVrFkTaWlpePnyJezs7OReZmZmud5PQRAgCAKSk5OzXF6jRg1UqlQJixcvRnp6utyykJAQnDhxAl27dpUrzzzw+9KlS+KlMhMTEwCQ2/+seoMsLS0xcOBA7NmzB6NGjcKff/6Z630iIiLlY+KjAlauXIm0tDTUrVsXu3fvRnh4OEJDQ7Fs2TKFyzkZ7O3tERMTg23btiEyMhLLli1T6E1p3Lgxrl27ho0bNyI8PBxTpkzBnTt3xOUymQx9+/bFmDFjcOrUKdy5cwd+fn5yg4sdHBzg6+uLnj17Ys+ePYiKisKVK1cwe/ZsHDx4MMvYHj58iNmzZ+P69euIiYlBUFAQOnXqBB0dHbRs2TLLdSQSCdatW4d79+6hQ4cOuHLlCmJiYrBz5060adMG9evXV3io4s6dO7F+/Xo8ePAAU6ZMwZUrVzB06FAAgJ2dHSwtLeHv74/w8HAcPHhQ4Y61ESNG4OjRo4iKisKNGzdw+vTpbMcYERHR98ExPirAxsYGN27cwMyZMzFq1Cg8f/4cJiYmcHFxwerVq7Ncp23bthg5ciSGDh2K5ORktGrVCpMmTYK/v79Yx8vLC5MmTcLYsWORlJSEPn36oGfPnrh9+7ZYZ/78+UhISECbNm2gr6+PUaNGIS4uTm5bAQEBmDFjBkaNGoWnT5+iTJky+OGHH9C6dessY9PW1sa5c+ewZMkSvHv3DmXLlkXDhg0RFBQEU1PTbI9DgwYNcOnSJUydOhUtWrTAhw8fUKFCBfTq1QsTJkyAVCqVqz916lRs27YNgwcPhrm5ObZu3YrKlSsDADQ1NbF161YMGjQI1apVQ506dTBjxgx06tRJXD8tLQ1DhgzBkydPYGBgAG9v7zzfFUdExZfHv2cLOwTKgkT4ltGvJVB8fDwMDQ0RFxcnN5A3KSkJUVFRsLa2znHwLJUMEokEe/fuVXhO0ffC842IKG+y+/7OjJe6iIiISGUw8SEiIiKVwTE+RFngFWAiopKJPT5ERESkMpj4EBERkcpg4pNHvARC3wPPMyKigsHEJ5c0NTUBAB8/fizkSEgVZJxnGecdEREpBwc355K6ujqMjIzw8uVLAICuri4kEkkhR0UljSAI+PjxI16+fAkjIyOoq6sXdkhERCUKE588yJg7KiP5ISooRkZGeZqrjIiIcqdYJD7R0dGYPn06Tp06hdjYWJQrVw7du3fHxIkToaWlJdaxtrZWWPfixYv44YcflBKHRCKBubk5TE1N8zwLOVFuaWpqsqeHiKiAFIvEJywsDOnp6Vi7di3s7Oxw584d9O/fH4mJiViwYIFc3RMnTqBKlSri+9KlSys9HnV1dX4xERERFUPFIvHx9vaGt7e3+N7Gxgb379/H6tWrFRKf0qVL8xIBERERZanY3tUVFxcHY2NjhfK2bdvC1NQUbm5u2L9/fyFERkREREVVsejxySwiIgLLly+X6+2RyWRYuHAhXF1doaamht27d8PHxwf79u1D27Zts20rOTkZycnJ4vv4+PgCjZ2IiIgKj0QoxCeljR8/HnPnzs2xTmhoKCpVqiS+f/r0KTw8PODp6Ym//vorx3V79uyJqKgonDt3Lts6/v7+mDp1qkL548ePc5zWnoiIiIqO+Ph4WFpa4v379zA0NMy2XqEmPq9evcKbN29yrGNjYyPeufXs2TN4enrihx9+QGBgINTUcr5St3LlSsyYMQPPnz/Ptk7mHp+nT5+icuXKedgLIiIiKioeP36M8uXLZ7u8UC91mZiYwMTEJFd1nz59ikaNGsHFxQUBAQFfTXoAIDg4GObm5jnWkUqlkEql4nuZTIbHjx9DX1+fDyj8BhmZN3vOqCjheUlFDc9J5REEAR8+fEC5cuVyrFcsxvg8ffoUnp6eqFixIhYsWIBXr16JyzLu4NqwYQO0tLRQs2ZNAMCePXuwfv36r14Oy0xNTS3HTJHyxsDAgL/MVOTwvKSihuekcuR0iStDsUh8jh8/joiICERERCgkJV9eqZs+fToePXoEDQ0NVKpUCdu3b0fHjh2/d7hERERURBXqGB8queLj42FoaIi4uDj+F0NFBs9LKmp4Tn5/xfY5PlS0SaVSTJkyRW78FFFh43lJRQ3Pye+PPT5ERESkMtjjQ0RERCqDiQ8RERGpDCY+REREpDKY+FCW/Pz8IJFIMGfOHLnyffv2yT3YURAE/PHHH6hXrx5kMhmMjIxQu3ZtLFmyBB8/fgQAfPz4ERMmTICtrS20tbVhYmICDw8P/PPPP2I7np6ekEgkCq+BAwd+nx2mYufVq1cYNGgQKlSoAKlUCjMzM3h5eeHChQsAACsrK0gkEmzbtk1h3SpVqkAikSAwMFAss7KywpIlSxTq+vv7o0aNGgW0F6QqYmNjMWzYMNjY2EAqlcLS0hJt2rTByZMn5erNnj0b6urqmD9/fiFFWvIVi+f4UOHQ1tbG3Llz8fPPP6NUqVJZ1unRowf27NmD33//HStWrICJiQlCQkKwZMkSWFlZwcfHBwMHDsTly5exfPlyVK5cGW/evEFQUJDCdCX9+/fHtGnT5Mp0dXULbP+oeOvQoQNSUlKwYcMG2NjY4MWLFzh58qTceWVpaYmAgAB06dJFLLt06RJiY2Ohp6dXGGGTCoqOjoarqyuMjIwwf/58ODs749OnTzh69CiGDBmCsLAwse769esxduxYrF+/HmPGjCnEqEsuJj6UraZNmyIiIgKzZ8/GvHnzFJbv2LEDW7Zswb59+9CuXTux3MrKCm3bthVnut+/fz+WLl2Kli1bistdXFwU2tPV1RWfxE2Uk/fv3+PcuXM4c+YMPDw8AAAVK1ZE3bp15er5+vpi8eLFePz4MSwtLQF8/mLx9fXFxo0bv3vcpJoGDx4MiUSCK1euyCXcVapUQZ8+fcT3Z8+exX///Ydp06Zh48aNCAoKQoMGDQoj5BKNl7ooW+rq6pg1axaWL1+OJ0+eKCzfsmULHB0d5ZKeDBKJRHx0uJmZGQ4dOoQPHz4UeMykGmQyGWQyGfbt2yc3yXBmZcuWhZeXFzZs2ADg82XX7du3y33ZEBWkt2/f4siRIxgyZEiWvYxGRkbiz+vWrUPXrl2hqamJrl27Yt26dd8xUtXBxIdy1L59e9SoUQNTpkxRWBYeHg5HR8evtvHHH38gKCgIpUuXRp06dTBy5EhxHMaXVq1aJX6hZby2bNmilP2gkkVDQwOBgYHYsGEDjIyM4Orqit9++w23bt1SqNunTx8EBgZCEATs2rULtra22Y7ZGTdunMI5OGvWrALeGyrJIiIiIAgCKlWqlGO9+Ph47Nq1C927dwcAdO/eHTt27EBCQsL3CFOlMPGhr5o7dy42bNiA0NBQufLcPvuyYcOGePjwIU6ePImOHTvi7t27cHd3x/Tp0+Xq+fr6Ijg4WO7Vtm1bpe0HlSwdOnTAs2fPsH//fnh7e+PMmTOoVauW3IBlAGjVqhUSEhLw77//Yv369Tn29owZM0bhHOQAe/oWuf07uXXrVtja2qJ69eoAgBo1aqBixYrYvn17QYankpj40Fc1bNgQXl5emDBhgly5g4OD3KC8nGhqasLd3R3jxo3DsWPHMG3aNEyfPh0pKSliHUNDQ9jZ2cm99PX1lbovVLJoa2ujWbNmmDRpEoKCguDn56fQO6mhoYEePXpgypQpuHz5Mnx9fbNtr0yZMgrnoLGxcUHvBpVg9vb2kEgkX/1buW7dOty9excaGhri6969e1i/fv13ilR1MPGhXJkzZw7+97//4eLFi2JZt27d8ODBA7nb0jMIgoC4uLhs26tcuTJSU1ORlJRUIPGSaqpcuTISExMVyvv06YOzZ8+iXbt22d6hSFQQjI2N4eXlhZUrV2Z5br5//x63b9/GtWvXcObMGbnexjNnzuDixYu5/geTcod3dVGuODs7w9fXF8uWLRPLOnfujL1796Jr1674/fff0bx5c5iYmOD27dtYvHgxhg0bBh8fH3h6eqJr166oXbs2SpcujXv37uG3335Do0aN5GYj/vjxI2JjY+W2K5VK+UVFCt68eYNOnTqhT58+qFatGvT19XHt2jXMmzcvy8H2Tk5OeP36NR+PQIVi5cqVcHV1Rd26dTFt2jRUq1YNqampOH78OFavXg0vLy/UrVsXDRs2VFi3Tp06WLduHZ/ro0Ts8aFcmzZtGtLT08X3EokEf//9NxYtWoR9+/bBw8MD1apVg7+/P9q1awcvLy8AEO+qad68OZycnDBs2DB4eXlhx44dcu3/+eefMDc3l3t17dr1u+4jFQ8ymQz16tXD4sWL0bBhQ1StWhWTJk1C//79sWLFiizXKV26NHR0dL5zpESAjY0Nbty4gUaNGmHUqFGoWrUqmjVrhpMnT2Lp0qXYvHkzOnTokOW6HTp0wMaNG/Hp06fvHHXJxdnZiYiISGWwx4eIiIhUBhMfIiIiUhlMfIiIiEhlMPEhIiIilcHEh4iIiFQGEx8iIiJSGUx8iIiISGUw8SEiIiKVwcSHiIiIVAYTHyIqEvz8/CCRSCCRSKCpqYmyZcuiWbNmWL9+vdxUKV8TGBgIIyOjggs0G35+fvDx8fnu2yWivGHiQ0RFhre3N54/f47o6GgcPnwYjRo1wi+//ILWrVsjNTW1sMMjohKAiQ8RFRlSqRRmZmawsLBArVq18Ntvv+Gff/7B4cOHERgYCABYtGgRnJ2doaenB0tLSwwePBgJCQkAgDNnzqB3796Ii4sTe4/8/f0BAJs2bULt2rWhr68PMzMzdOvWDS9fvhS3/e7dO/j6+sLExAQ6Ojqwt7dHQECAuPzx48fo3LkzjIyMYGxsjHbt2iE6OhoA4O/vjw0bNuCff/4Rt3vmzJnvcciIKI+Y+BBRkda4cWNUr14de/bsAQCoqalh2bJluHv3LjZs2IBTp05h7NixAIAGDRpgyZIlMDAwwPPnz/H8+XOMHj0aAPDp0ydMnz4dISEh2LdvH6Kjo+Hn5yduZ9KkSbh37x4OHz6M0NBQrF69GmXKlBHX9fLygr6+Ps6dO4cLFy5AJpPB29sbKSkpGD16NDp37iz2WD1//hwNGjT4vgeKiHJFo7ADICL6mkqVKuHWrVsAgBEjRojlVlZWmDFjBgYOHIhVq1ZBS0sLhoaGkEgkMDMzk2ujT58+4s82NjZYtmwZ6tSpg4SEBMhkMsTExKBmzZqoXbu22HaG7du3Iz09HX/99RckEgkAICAgAEZGRjhz5gyaN28OHR0dJCcnK2yXiIoW9vgQUZEnCIKYcJw4cQJNmjSBhYUF9PX10aNHD7x58wYfP37MsY3r16+jTZs2qFChAvT19eHh4QEAiImJAQAMGjQI27ZtQ40aNTB27FgEBQWJ64aEhCAiIgL6+vqQyWSQyWQwNjZGUlISIiMjC2iviaggMPEhoiIvNDQU1tbWiI6ORuvWrVGtWjXs3r0b169fx8qVKwEAKSkp2a6fmJgILy8vGBgYYMuWLbh69Sr27t0rt16LFi3w6NEjjBw5Es+ePUOTJk3Ey2QJCQlwcXFBcHCw3OvBgwfo1q1bAe89ESkTL3URUZF26tQp3L59GyNHjsT169eRnp6OhQsXQk3t8/9tO3bskKuvpaWFtLQ0ubKwsDC8efMGc+bMgaWlJQDg2rVrCtsyMTFBr1690KtXL7i7u2PMmDFYsGABatWqhe3bt8PU1BQGBgZZxpnVdomo6GGPDxEVGcnJyYiNjcXTp09x48YNzJo1C+3atUPr1q3Rs2dP2NnZ4dOnT1i+fDkePnyITZs2Yc2aNXJtWFlZISEhASdPnsTr16/x8eNHVKhQAVpaWuJ6+/fvx/Tp0+XWmzx5Mv755x9ERETg7t27OHDgAJycnAAAvr6+KFOmDNq1a4dz584hKioKZ86cwfDhw/HkyRNxu7du3cL9+/fx+vVrfPr06fscNCLKG4GIqAjo1auXAEAAIGhoaAgmJiZC06ZNhfXr1wtpaWlivUWLFgnm5uaCjo6O4OXlJWzcuFEAILx7906sM3DgQKF06dICAGHKlCmCIAjC33//LVhZWQlSqVSoX7++sH//fgGAcPPmTUEQBGH69OmCk5OToKOjIxgbGwvt2rUTHj58KLb5/PlzoWfPnkKZMmUEqVQq2NjYCP379xfi4uIEQRCEly9fCs2aNRNkMpkAQDh9+nRBHzIiygeJIAhCYSZeRERERN8LL3URERGRymDiQ0RERCqDiQ8RERGpDCY+REREpDKY+BAREZHKYOJDREREKoOJDxEREakMJj5ERESkMpj4EBERkcpg4kNEREQqg4kPERERqQwmPkRERKQy/h+snsqP9YRIlQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(data = df_merged, x = 'Dataset', y = 'Difference', hue = 'Model')\n",
    "plt.title(\"Difference between 'Full' and 'Instruct' prompt\\nin terms of Error Reduction Percentage\")\n",
    "\n",
    "plt.savefig(os.path.join(save_appendix,\"prompt_comparison.pdf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
