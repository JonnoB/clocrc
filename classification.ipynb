{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class and genre classification\n",
    "\n",
    "\n",
    "This note book focuses on classifying the corrected documents into genre and IPTC class\n",
    "\n",
    "The approach taken in this notebook is that classifying the text can be performed by a much simpler model, as such the train set will be used to generate a silver label set of data on which a BERT model will be trained. There will be two models\n",
    "\n",
    "- BERT G: Classifies genres\n",
    "- BERT T: Classifies Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import config  # Import your config.py file this contains you openai api key\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from llm_comparison_toolkit import RateLimiter, get_response_openai, get_response_anthropic,  create_config_dict_func, use_df_to_call_llm_api, compare_request_configurations\n",
    "from evaluation_funcs import evaluate_correction_performance, evaluate_correction_performance_folders, get_metric_error_reduction\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from helper_functions import files_to_df_func\n",
    "\n",
    "dev_transcripts = 'data/dev_data_transcript'\n",
    "\n",
    "#load the dev and test sets for prompt development and selection\n",
    "dev_data_df = pd.read_csv('data/dev_data_raw.csv')\n",
    "test_data_df = pd.read_csv('data/test_data_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Genre classes\n",
    "\n",
    "This paper uses the below classes to categorise the article types. As can be seen definition is left up to the LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_prompt = \"\"\"\n",
    "Read following article.\n",
    ":::\n",
    "{truncated_text}\n",
    ":::\n",
    "\n",
    "You are a machine that classifies newspaper articles. Your response  is limited to choices from the following json\n",
    "        {0: 'news report',\n",
    "        1: 'editorial',\n",
    "        2: 'letter',\n",
    "        3: 'advert',\n",
    "        4: 'review',\n",
    "        5: 'poem/song/story',\n",
    "        6: 'other'}\n",
    "        you will respond using a single digit.\n",
    "\n",
    "    For example given the text \"Mr Bronson died today, he was a kind man\" your answer would be\n",
    "    6\n",
    "    \n",
    "    Alternatively given the text \"The prime minster spoke at parliament today\" your answer would be\n",
    "    0\n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IPTC classes\n",
    "\n",
    "THe below prompt is for the IPTC Topic classes using the level one classification nomenclature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPTC_prompt = \"\"\"\n",
    "Read following article.\n",
    ":::\n",
    "{truncated_text}\n",
    ":::\n",
    "\n",
    "You are a machine that classifies newspaper articles. Your response  is limited to choices from the following json\n",
    "    {0: 'arts, culture, entertainment and media',\n",
    "    1: 'crime, law and justice',\n",
    "    2: 'disaster, accident and emergency incident',\n",
    "    3: 'economy, business and finance',\n",
    "    4: 'education',\n",
    "    5: 'environment',\n",
    "    6: 'health',\n",
    "    7: 'human interest',\n",
    "    8: 'labour',\n",
    "    9: 'lifestyle and leisure',\n",
    "    10: 'politics',\n",
    "    11: 'religion',\n",
    "    12: 'science and technology',\n",
    "    13: 'society',\n",
    "    14: 'sport',\n",
    "    15: 'conflict, war and peace',\n",
    "    16: 'weather'}\n",
    "    you will respond using a numeric python list.\n",
    "\n",
    "For example given the text \"The War with spain has forced schools to close\" your answer would be\n",
    "[15,4]\n",
    "\n",
    "Alternatively given the text \"The prime minster spoke at parliament today\" your answer would be\n",
    "[10]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating model configs\n",
    "\n",
    "For simplicity the same functions used for cleaning the ocr will be used here, this means that each text will create it's own file containing at most a few numbers. There are obviously more efficient ways of doing this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = dev_transcripts\n",
    "\n",
    "df = files_to_df_func(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/redigitalize/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 35\u001b[0m\n\u001b[1;32m     21\u001b[0m     row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madditional_args\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_name\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenre\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     22\u001b[0m     genre_model_configs\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m         create_config_dict_func(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m     )\n\u001b[0;32m---> 35\u001b[0m \u001b[43mcompare_request_configurations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenre_model_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data/dev_genre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/redigitalize/llm_comparison_toolkit.py:297\u001b[0m, in \u001b[0;36mcompare_request_configurations\u001b[0;34m(df, configurations, folder_path)\u001b[0m\n\u001b[1;32m    294\u001b[0m response_name \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresponse_name\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# Call perform_capoc with the current configuration\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[43muse_df_to_call_llm_api\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_name\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/redigitalize/llm_comparison_toolkit.py:249\u001b[0m, in \u001b[0;36muse_df_to_call_llm_api\u001b[0;34m(config_dict, df, response_name, folder_path)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;66;03m# List to accumulate new records\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m#new_records = []\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_ids:\n\u001b[1;32m    250\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Begin measuring time\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         corrected_ocr \u001b[38;5;241m=\u001b[39m make_api_call_from_dataframe_row(row, config_dict) \n",
      "File \u001b[0;32m~/redigitalize/.venv/lib/python3.11/site-packages/pandas/core/series.py:1040\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "File \u001b[0;32m~/redigitalize/.venv/lib/python3.11/site-packages/pandas/core/series.py:1156\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1156\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[1;32m   1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[loc]\n",
      "File \u001b[0;32m~/redigitalize/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "groq_alt_endpoint = {'alt_endpoint':{'base_url':'https://api.groq.com/openai/v1',\n",
    "                     'api_key':os.getenv(\"GROQ_API_KEY\")}}\n",
    "\n",
    "basic_model_configs = pd.DataFrame({\n",
    "    'get_response_func': [get_response_openai, get_response_openai, get_response_anthropic, get_response_anthropic, \n",
    "                          get_response_openai, get_response_openai, get_response_openai], \n",
    "    'engine': ['gpt-3.5-turbo', 'gpt-4-turbo-preview', \"claude-3-haiku-20240307\", \"claude-3-opus-20240229\", \n",
    "               'mixtral-8x7b-32768', 'llama2-70b-4096', 'gemma-7b-it'],\n",
    "    'additional_args': [\n",
    "        {}, {}, {}, {}, \n",
    "        groq_alt_endpoint, \n",
    "        groq_alt_endpoint, \n",
    "        groq_alt_endpoint\n",
    "    ]\n",
    "})\n",
    "\n",
    "genre_model_configs= []\n",
    "\n",
    "for index, row in basic_model_configs.iterrows():\n",
    "    #modify the response name for the type\n",
    "    row['additional_args']['response_name'] = 'genre'\n",
    "    genre_model_configs.append(\n",
    "\n",
    "        create_config_dict_func(\n",
    "    get_response_func = row['get_response_func'],\n",
    "    rate_limiter = RateLimiter(40000),\n",
    "    engine = row['engine'],\n",
    "    system_message_template = \"\",\n",
    "    prompt_template =  genre_prompt,\n",
    "    additional_args=row['additional_args']\n",
    "    )\n",
    "\n",
    "    )\n",
    "\n",
    "compare_request_configurations(df, genre_model_configs, folder_path='./data/dev_genre')\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
